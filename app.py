import warnings
warnings.filterwarnings("ignore", message="missing ScriptRunContext")
import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.svm import SVC, SVR
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, mean_squared_error, mean_absolute_error, classification_report, confusion_matrix
import xgboost as xgb
import lightgbm as lgb
from imblearn.over_sampling import SMOTE
import shap
import joblib
import warnings
from datetime import datetime, timedelta
import json
import sqlite3
import hashlib
from collections import deque
import threading
import time

# Handle optional dependencies with warnings
try:
    import statsmodels
    STATSMODELS_AVAILABLE = True
except ImportError:
    STATSMODELS_AVAILABLE = False
    print("Warning: statsmodels not available. Some advanced time series features will be limited.")

try:
    from prophet import Prophet
    PROPHET_AVAILABLE = True
except ImportError:
    PROPHET_AVAILABLE = False
    print("Warning: Prophet not available. Facebook Prophet forecasting will be disabled.")

# Import streamlit first
import streamlit as st

# Import our advanced modules
try:
    from real_time_monitor import RealTimeMonitor, create_realtime_dashboard, ThreatIntelligenceIntegrator
    from time_series_analysis import CyberAttackTimeSeriesAnalyzer, create_time_series_dashboard
    from auto_retrain import AutoRetrainingOrchestrator, create_auto_retrain_dashboard
    ADVANCED_MODULES_AVAILABLE = True
except ImportError as e:
    st.error(f"Advanced modules not available: {e}")
    ADVANCED_MODULES_AVAILABLE = False

warnings.filterwarnings('ignore')

# Page configuration
st.set_page_config(
    page_title="Cybersecurity Attack Predictor",
    page_icon="üõ°Ô∏è",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS for better styling
st.markdown("""
<style>
.main-header {
    font-size: 3rem;
    color: #1f77b4;
    text-align: center;
    margin-bottom: 2rem;
    font-weight: bold;
}
.sub-header {
    font-size: 1.5rem;
    color: #ff7f0e;
    margin-bottom: 1rem;
}
.metric-card {
    background-color: #f0f2f6;
    padding: 1rem;
    border-radius: 0.5rem;
    margin: 0.5rem 0;
}
.prediction-result {
    background-color: #d4edda;
    border: 1px solid #c3e6cb;
    border-radius: 0.25rem;
    padding: 1rem;
    margin: 1rem 0;
}
.warning-result {
    background-color: #f8d7da;
    border: 1px solid #f5c6cb;
    border-radius: 0.25rem;
    padding: 1rem;
    margin: 1rem 0;
}
</style>
""", unsafe_allow_html=True)

# Initialize session state
if 'models_trained' not in st.session_state:
    st.session_state.models_trained = False
if 'data_loaded' not in st.session_state:
    st.session_state.data_loaded = False
if 'predictor' not in st.session_state:
    st.session_state.predictor = None
if 'monitor' not in st.session_state and ADVANCED_MODULES_AVAILABLE:
    st.session_state.monitor = RealTimeMonitor()
if 'ts_analyzer' not in st.session_state and ADVANCED_MODULES_AVAILABLE:
    st.session_state.ts_analyzer = CyberAttackTimeSeriesAnalyzer()
if 'auto_retrain' not in st.session_state and ADVANCED_MODULES_AVAILABLE:
    st.session_state.auto_retrain = AutoRetrainingOrchestrator()
if 'threat_intel' not in st.session_state and ADVANCED_MODULES_AVAILABLE:
    st.session_state.threat_intel = ThreatIntelligenceIntegrator()

# Sidebar Navigation
st.sidebar.title("üõ°Ô∏è Cyber Defense Suite")
st.sidebar.markdown("### Advanced Cybersecurity Analytics")

# Main navigation
page = st.sidebar.selectbox(
    "üéØ Select Module",
    [
        "üè† Executive Dashboard",
        "üìä Data Analytics & Visualization", 
        "ü§ñ ML Model Training & Management",
        "üîÆ Attack Prediction Engine",
        "üî¥ Real-Time Threat Monitoring",
        "üìà Time-Series Analysis & Forecasting",
        "üîÑ Auto-Retraining System",
        "üß† Explainable AI & Model Insights",
        "üåê Threat Intelligence Integration",
        "üìã Incident Response & Reporting",
        "‚öôÔ∏è System Configuration"
    ]
)

# Sub-navigation based on main selection
if page == "üìä Data Analytics & Visualization":
    sub_page = st.sidebar.selectbox(
        "üìä Analytics Module",
        ["Data Overview", "Correlation Analysis", "Geographic Analysis", "Industry Analysis", "Anomaly Detection"]
    )
elif page == "ü§ñ ML Model Training & Management":
    sub_page = st.sidebar.selectbox(
        "ü§ñ ML Module",
        ["Model Training", "Model Comparison", "Hyperparameter Tuning", "Feature Engineering", "Model Deployment"]
    )
elif page == "üîÆ Attack Prediction Engine":
    sub_page = st.sidebar.selectbox(
        "üîÆ Prediction Module",
        ["Single Prediction", "Batch Prediction", "Risk Assessment", "Scenario Analysis", "What-If Analysis"]
    )
elif page == "üî¥ Real-Time Threat Monitoring":
    sub_page = st.sidebar.selectbox(
        "üî¥ Monitoring Module",
        ["Live Dashboard", "Alert Management", "Threat Hunting", "Network Analysis", "IoC Management"]
    )
elif page == "üìà Time-Series Analysis & Forecasting":
    sub_page = st.sidebar.selectbox(
        "üìà Time-Series Module",
        ["Trend Analysis", "Seasonal Patterns", "Forecasting", "Anomaly Detection", "Risk Projection"]
    )
else:
    sub_page = None

class CyberSecurityPredictor:
    def __init__(self):
        self.data = None
        self.models = {}
        self.scalers = {}
        self.encoders = {}
        self.feature_columns = None
        
    def load_data(self, file_path=None, uploaded_file=None):
        """Load cybersecurity data"""
        try:
            if uploaded_file is not None:
                self.data = pd.read_csv(uploaded_file)
            elif file_path:
                self.data = pd.read_csv(file_path)
            else:
                # Default dataset
                self.data = pd.read_csv('Global_Cybersecurity_Threats_2015-2024.csv')
            
            st.session_state.data_loaded = True
            return True
        except Exception as e:
            st.error(f"Error loading data: {str(e)}")
            return False
    
    def preprocess_data(self):
        """Preprocess the cybersecurity data"""
        if self.data is None:
            return False
            
        # Create a copy for processing
        df = self.data.copy()
        
        # Handle missing values
        df = df.dropna()
        
        # Create severity score based on financial loss and affected users
        df['Severity_Score'] = (
            df['Financial Loss (in Million $)'] * 0.6 + 
            (df['Number of Affected Users'] / 1000000) * 0.4
        )
        
        # Create time-based features
        df['Attack_Year'] = df['Year']
        df['Years_Since_2015'] = df['Year'] - 2015
        
        # Encode categorical variables
        categorical_columns = ['Country', 'Attack Type', 'Target Industry', 
                             'Attack Source', 'Security Vulnerability Type', 
                             'Defense Mechanism Used']
        
        self.encoders = {}
        for col in categorical_columns:
            le = LabelEncoder()
            df[f'{col}_encoded'] = le.fit_transform(df[col])
            self.encoders[col] = le
        
        # Define feature columns
        self.feature_columns = [
            'Country_encoded', 'Target Industry_encoded', 'Attack Source_encoded',
            'Security Vulnerability Type_encoded', 'Defense Mechanism Used_encoded',
            'Years_Since_2015', 'Number of Affected Users'
        ]
        
        # Scale numerical features
        scaler = StandardScaler()
        numerical_cols = ['Financial Loss (in Million $)', 'Number of Affected Users', 
                         'Incident Resolution Time (in Hours)', 'Years_Since_2015']
        
        df[numerical_cols] = scaler.fit_transform(df[numerical_cols])
        self.scalers['numerical'] = scaler
        
        self.processed_data = df
        return True
    
    def train_models(self):
        """Train multiple ML models for different prediction tasks"""
        if not hasattr(self, 'processed_data'):
            return False
        
        df = self.processed_data
        X = df[self.feature_columns]
        
        # Task 1: Attack Type Classification
        y_attack_type = df['Attack Type_encoded']
        X_train, X_test, y_train, y_test = train_test_split(X, y_attack_type, test_size=0.2, random_state=42)
        
        # Handle class imbalance
        smote = SMOTE(random_state=42)
        X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)
        
        # Random Forest for Attack Type
        rf_attack = RandomForestClassifier(n_estimators=100, random_state=42)
        rf_attack.fit(X_train_balanced, y_train_balanced)
        
        # XGBoost for Attack Type
        xgb_attack = xgb.XGBClassifier(random_state=42)
        xgb_attack.fit(X_train_balanced, y_train_balanced)
        
        self.models['attack_type_rf'] = rf_attack
        self.models['attack_type_xgb'] = xgb_attack
        
        # Task 2: Financial Loss Regression
        y_financial = df['Financial Loss (in Million $)']
        X_train, X_test, y_train, y_test = train_test_split(X, y_financial, test_size=0.2, random_state=42)
        
        rf_financial = RandomForestRegressor(n_estimators=100, random_state=42)
        rf_financial.fit(X_train, y_train)
        
        xgb_financial = xgb.XGBRegressor(random_state=42)
        xgb_financial.fit(X_train, y_train)
        
        self.models['financial_rf'] = rf_financial
        self.models['financial_xgb'] = xgb_financial
        
        # Task 3: Resolution Time Regression
        y_resolution = df['Incident Resolution Time (in Hours)']
        X_train, X_test, y_train, y_test = train_test_split(X, y_resolution, test_size=0.2, random_state=42)
        
        rf_resolution = RandomForestRegressor(n_estimators=100, random_state=42)
        rf_resolution.fit(X_train, y_train)
        
        self.models['resolution_rf'] = rf_resolution
        
        # Task 4: Severity Classification
        df['Severity_Level'] = pd.cut(df['Severity_Score'], 
                                    bins=[0, 25, 50, 75, 100], 
                                    labels=['Low', 'Medium', 'High', 'Critical'])
        
        y_severity = LabelEncoder().fit_transform(df['Severity_Level'].dropna())
        X_severity = X.iloc[:len(y_severity)]
        
        X_train, X_test, y_train, y_test = train_test_split(X_severity, y_severity, test_size=0.2, random_state=42)
        
        rf_severity = RandomForestClassifier(n_estimators=100, random_state=42)
        rf_severity.fit(X_train, y_train)
        
        self.models['severity_rf'] = rf_severity
        self.encoders['severity'] = LabelEncoder().fit(['Low', 'Medium', 'High', 'Critical'])
        
        st.session_state.models_trained = True
        return True
    
    def predict_attack(self, input_data):
        """Make predictions for new attack data"""
        predictions = {}
        
        # Attack Type Prediction
        try:
            if 'attack_type_rf' in self.models:
                attack_pred = self.models['attack_type_rf'].predict([input_data])[0]
                attack_type = self.encoders['Attack Type'].inverse_transform([attack_pred])[0]
                predictions['attack_type'] = attack_type
        except:
            predictions['attack_type'] = 'Unknown'
        
        # Financial Loss Prediction
        try:
            if 'financial_rf' in self.models:
                financial_pred = self.models['financial_rf'].predict([input_data])[0]
                predictions['financial_loss'] = max(0, financial_pred * 100)
        except:
            predictions['financial_loss'] = 0
        
        # Resolution Time Prediction
        try:
            if 'resolution_rf' in self.models:
                resolution_pred = self.models['resolution_rf'].predict([input_data])[0]
                predictions['resolution_time'] = max(0, resolution_pred * 24)
        except:
            predictions['resolution_time'] = 0
        
        # Severity Level Prediction
        try:
            if 'severity_rf' in self.models:
                severity_pred = self.models['severity_rf'].predict([input_data])[0]
                severity_levels = ['Low', 'Medium', 'High', 'Critical']
                predictions['severity_level'] = severity_levels[min(severity_pred, 3)]
        except:
            predictions['severity_level'] = 'Medium'
        
        return predictions

# Main Application Router
def main():
    """Main application with comprehensive navigation"""
    
    # Initialize predictor
    if st.session_state.predictor is None:
        st.session_state.predictor = CyberSecurityPredictor()
    
    predictor = st.session_state.predictor
    
    # Route to appropriate page
    if page == "üè† Executive Dashboard":
        show_executive_dashboard(predictor)
    elif page == "üìä Data Analytics & Visualization":
        show_data_analytics(predictor, sub_page)
    elif page == "ü§ñ ML Model Training & Management":
        show_ml_management(predictor, sub_page)
    elif page == "üîÆ Attack Prediction Engine":
        show_prediction_engine(predictor, sub_page)
    elif page == "üî¥ Real-Time Threat Monitoring" and ADVANCED_MODULES_AVAILABLE:
        show_real_time_monitoring(sub_page)
    elif page == "üìà Time-Series Analysis & Forecasting" and ADVANCED_MODULES_AVAILABLE:
        show_time_series_analysis(sub_page)
    elif page == "üîÑ Auto-Retraining System" and ADVANCED_MODULES_AVAILABLE:
        show_auto_retraining()
    elif page == "üß† Explainable AI & Model Insights":
        show_explainable_ai(predictor)
    elif page == "üåê Threat Intelligence Integration" and ADVANCED_MODULES_AVAILABLE:
        show_threat_intelligence()
    elif page == "üìã Incident Response & Reporting":
        show_incident_response(predictor)
    elif page == "‚öôÔ∏è System Configuration":
        show_system_configuration()

def show_executive_dashboard(predictor):
    """Executive-level dashboard with KPIs and high-level insights"""
    st.markdown("# üè† Executive Dashboard")
    st.markdown("### Real-time Cybersecurity Threat Intelligence")
    
    # Load data if available
    if not st.session_state.data_loaded:
        try:
            predictor.load_data()
            predictor.preprocess_data()
        except Exception as e:
            st.warning("Please upload data to see dashboard metrics")
            st.error(f"Error: {str(e)}")
            return
    
    if predictor.data is not None:
        # Key Performance Indicators
        col1, col2, col3, col4, col5 = st.columns(5)
        
        with col1:
            total_incidents = len(predictor.data)
            st.metric("Total Incidents", f"{total_incidents:,}")
        
        with col2:
            total_loss = predictor.data['Financial Loss (in Million $)'].sum()
            st.metric("Total Financial Loss", f"${total_loss:.1f}M")
        
        with col3:
            avg_resolution = predictor.data['Incident Resolution Time (in Hours)'].mean()
            st.metric("Avg Resolution Time", f"{avg_resolution:.1f}h")
        
        with col4:
            total_users = predictor.data['Number of Affected Users'].sum()
            st.metric("Users Affected", f"{total_users:,.0f}")
        
        with col5:
            unique_countries = predictor.data['Country'].nunique()
            st.metric("Countries Affected", unique_countries)
        
        # Threat Level Gauge
        st.markdown("### üö® Current Threat Level")
        current_threat_level = calculate_global_threat_level(predictor.data)
        create_threat_gauge(current_threat_level)
        
        # Top Attack Types
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("### üéØ Top Attack Types")
            attack_counts = predictor.data['Attack Type'].value_counts().head(5)
            fig = px.bar(x=attack_counts.values, y=attack_counts.index, 
                        orientation='h', title="Most Common Attack Types")
            fig.update_layout(height=400)
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            st.markdown("### üåç Geographic Threat Distribution")
            country_threats = predictor.data.groupby('Country')['Financial Loss (in Million $)'].sum().head(10)
            fig = px.bar(x=country_threats.index, y=country_threats.values,
                        title="Financial Impact by Country")
            fig.update_layout(height=400, xaxis_tickangle=-45)
            st.plotly_chart(fig, use_container_width=True)
        
        # Monthly Trend
        st.markdown("### üìà Monthly Incident Trends")
        yearly_data = predictor.data.groupby('Year').agg({
            'Financial Loss (in Million $)': 'sum',
            'Number of Affected Users': 'sum',
            'Year': 'count'
        }).rename(columns={'Year': 'Incident Count'})
        
        fig = make_subplots(specs=[[{"secondary_y": True}]])
        
        fig.add_trace(
            go.Scatter(x=yearly_data.index, y=yearly_data['Incident Count'],
                      mode='lines+markers', name='Incidents', line=dict(color='blue', width=3)),
            secondary_y=False,
        )
        
        fig.add_trace(
            go.Scatter(x=yearly_data.index, y=yearly_data['Financial Loss (in Million $)'],
                      mode='lines+markers', name='Financial Loss ($M)', line=dict(color='red', width=3)),
            secondary_y=True,
        )
        
        fig.update_xaxes(title_text="Year")
        fig.update_yaxes(title_text="Number of Incidents", secondary_y=False)
        fig.update_yaxes(title_text="Financial Loss (Million $)", secondary_y=True)
        fig.update_layout(title="Cybersecurity Incidents Trend", height=500)
        
        st.plotly_chart(fig, use_container_width=True)
        
        # Risk Heatmap
        st.markdown("### üó∫Ô∏è Industry Risk Heatmap")
        risk_matrix = predictor.data.pivot_table(
            values='Financial Loss (in Million $)',
            index='Target Industry',
            columns='Attack Type',
            aggfunc='mean',
            fill_value=0
        )
        
        fig = px.imshow(risk_matrix.values,
                       x=risk_matrix.columns,
                       y=risk_matrix.index,
                       aspect="auto",
                       title="Average Financial Loss by Industry and Attack Type")
        fig.update_layout(height=600)
        st.plotly_chart(fig, use_container_width=True)

def calculate_global_threat_level(data):
    """Calculate overall threat level based on recent data"""
    # Simple threat level calculation based on recent trends
    recent_data = data[data['Year'] >= data['Year'].max() - 1]
    
    if len(recent_data) == 0:
        return 50
    
    # Factors: financial loss, affected users, incident frequency
    avg_loss = recent_data['Financial Loss (in Million $)'].mean()
    avg_users = recent_data['Number of Affected Users'].mean()
    incident_count = len(recent_data)
    
    # Normalize and combine factors (0-100 scale)
    loss_score = min(avg_loss / 10, 40)
    user_score = min(avg_users / 1000000 * 30, 30)
    frequency_score = min(incident_count / 10 * 30, 30)
    
    return min(loss_score + user_score + frequency_score, 100)

def create_threat_gauge(threat_level):
    """Create a threat level gauge"""
    fig = go.Figure(go.Indicator(
        mode = "gauge+number+delta",
        value = threat_level,
        domain = {'x': [0, 1], 'y': [0, 1]},
        title = {'text': "Global Threat Level"},
        delta = {'reference': 50},
        gauge = {
            'axis': {'range': [None, 100]},
            'bar': {'color': "darkblue"},
            'steps': [
                {'range': [0, 25], 'color': "lightgreen"},
                {'range': [25, 50], 'color': "yellow"},
                {'range': [50, 75], 'color': "orange"},
                {'range': [75, 100], 'color': "red"}
            ],
            'threshold': {
                'line': {'color': "red", 'width': 4},
                'thickness': 0.75,
                'value': 90
            }
        }
    ))
    
    fig.update_layout(height=400)
    st.plotly_chart(fig, use_container_width=True)

# Function implementations
def show_data_analytics(predictor, sub_page):
    """Data analytics and visualization module"""
    st.markdown("# üìä Data Analytics & Visualization")
    
    if predictor.data is None:
        if not predictor.load_data():
            st.error("Failed to load data")
            return
        predictor.preprocess_data()
    
    if sub_page == "Data Overview":
        show_data_overview(predictor)
    elif sub_page == "Correlation Analysis":
        show_correlation_analysis(predictor)
    elif sub_page == "Geographic Analysis":
        show_geographic_analysis(predictor)
    elif sub_page == "Industry Analysis":
        show_industry_analysis(predictor)
    elif sub_page == "Anomaly Detection":
        show_anomaly_detection(predictor)

def show_ml_management(predictor, sub_page):
    """ML model training and management module"""
    st.markdown("# ü§ñ ML Model Training & Management")
    
    if predictor.data is None:
        st.warning("Please load data first")
        return
        
    if sub_page == "Model Training":
        show_model_training(predictor)
    elif sub_page == "Model Comparison":
        show_model_comparison(predictor)
    elif sub_page == "Hyperparameter Tuning":
        show_hyperparameter_tuning(predictor)
    elif sub_page == "Feature Engineering":
        show_feature_engineering(predictor)
    elif sub_page == "Model Deployment":
        show_model_deployment(predictor)

def show_prediction_engine(predictor, sub_page):
    """Attack prediction engine module"""
    st.markdown("# üîÆ Attack Prediction Engine")
    
    if not st.session_state.models_trained:
        st.warning("Please train models first")
        return
        
    if sub_page == "Single Prediction":
        show_single_prediction(predictor)
    elif sub_page == "Batch Prediction":
        show_batch_prediction(predictor)
    elif sub_page == "Risk Assessment":
        show_risk_assessment(predictor)
    elif sub_page == "Scenario Analysis":
        show_scenario_analysis(predictor)
    elif sub_page == "What-If Analysis":
        show_what_if_analysis(predictor)

def show_real_time_monitoring(sub_page):
    """Real-time threat monitoring module"""
    st.markdown("# üî¥ Real-Time Threat Monitoring")
    
    if sub_page == "Live Dashboard":
        if ADVANCED_MODULES_AVAILABLE:
            try:
                create_realtime_dashboard()  # Call without arguments
            except Exception as e:
                st.error(f"Error creating real-time dashboard: {e}")
                st.info("Real-time monitoring dashboard - Feature in development")
        else:
            st.warning("Advanced modules not available. Real-time monitoring disabled.")
    elif sub_page == "Alert Management":
        show_alert_management()
    elif sub_page == "Threat Hunting":
        show_threat_hunting()
    elif sub_page == "Network Analysis":
        show_network_analysis()
    elif sub_page == "IoC Management":
        show_ioc_management()

def show_time_series_analysis(sub_page):
    """Time-series analysis and forecasting module"""
    st.markdown("# üìà Time-Series Analysis & Forecasting")
    
    if sub_page == "Trend Analysis":
        if ADVANCED_MODULES_AVAILABLE:
            try:
                # Create sample data for time series analysis
                predictor = st.session_state.predictor
                if predictor and predictor.data is not None:
                    create_time_series_dashboard(st.session_state.ts_analyzer, predictor.data)
                else:
                    st.warning("Please load data first for time series analysis.")
            except Exception as e:
                st.error(f"Error creating time series dashboard: {e}")
                st.info("Time series analysis dashboard - Feature in development")
        else:
            st.warning("Advanced modules not available. Time series analysis disabled.")
    elif sub_page == "Seasonal Patterns":
        show_seasonal_patterns()
    elif sub_page == "Forecasting":
        show_forecasting()
    elif sub_page == "Anomaly Detection":
        show_ts_anomaly_detection()
    elif sub_page == "Risk Projection":
        show_risk_projection()

def show_auto_retraining():
    """Auto-retraining system module"""
    st.markdown("# üîÑ Auto-Retraining System")
    
    if ADVANCED_MODULES_AVAILABLE:
        try:
            create_auto_retrain_dashboard(st.session_state.auto_retrain)
        except Exception as e:
            st.error(f"Error creating auto-retrain dashboard: {e}")
            st.info("Auto-retraining system dashboard - Feature in development")
    else:
        st.warning("Advanced modules not available. Auto-retraining disabled.")
        
        # Show basic retraining info
        st.markdown("### üîÑ Manual Retraining Options")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("Models Trained", "4" if st.session_state.models_trained else "0")
        with col2:
            st.metric("Last Training", "Today" if st.session_state.models_trained else "Never")
        with col3:
            st.metric("Training Status", "Ready" if st.session_state.models_trained else "Pending")
        
        if st.button("Trigger Manual Retraining", type="primary"):
            if st.session_state.predictor and st.session_state.predictor.data is not None:
                with st.spinner("Retraining models..."):
                    success = st.session_state.predictor.train_models()
                    if success:
                        st.success("‚úÖ Models retrained successfully!")
                    else:
                        st.error("‚ùå Error during retraining.")
            else:
                st.warning("Please load data first.")

def show_explainable_ai(predictor):
    """Explainable AI and model insights module"""
    st.markdown("# üß† Explainable AI & Model Insights")
    model_insights_page(predictor)

def show_threat_intelligence():
    """Threat intelligence integration module"""
    st.markdown("# üåê Threat Intelligence Integration")
    
    if ADVANCED_MODULES_AVAILABLE:
        try:
            st.session_state.threat_intel.create_dashboard()
        except Exception as e:
            st.error(f"Error creating threat intelligence dashboard: {e}")
            st.info("Threat intelligence integration - Feature in development")
    else:
        st.warning("Advanced modules not available. Threat intelligence disabled.")
        
        # Show basic threat intelligence info
        st.markdown("### üåê Threat Intelligence Overview")
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("Threat Feeds", "5")
        with col2:
            st.metric("IoCs Tracked", "1,247")
        with col3:
            st.metric("Last Update", "2 hrs ago")
        with col4:
            st.metric("Active Threats", "23")
        
        # Mock threat intelligence data
        st.markdown("### Recent Threat Intelligence")
        
        threat_data = {
            'Indicator': ['192.168.1.100', 'malware.exe', 'phishing-site.com', 'suspicious-domain.net'],
            'Type': ['IP Address', 'File Hash', 'Domain', 'Domain'],
            'Threat Level': ['High', 'Critical', 'Medium', 'High'],
            'Source': ['OSINT', 'Internal', 'Commercial Feed', 'OSINT'],
            'Last Seen': ['2024-01-15', '2024-01-14', '2024-01-13', '2024-01-12']
        }
        
        threat_df = pd.DataFrame(threat_data)
        st.dataframe(threat_df, use_container_width=True)
        
        # Threat intelligence settings
        st.markdown("### Configuration")
        
        with st.expander("Threat Feed Settings"):
            feed_types = st.multiselect(
                "Select Threat Feeds",
                ["OSINT", "Commercial", "Government", "Industry Sharing"],
                default=["OSINT", "Commercial"]
            )
            
            update_frequency = st.selectbox(
                "Update Frequency",
                ["Every Hour", "Every 6 Hours", "Daily", "Weekly"]
            )
            
            if st.button("Update Configuration"):
                st.success("Threat intelligence configuration updated!")

def show_incident_response(predictor):
    """Incident response and reporting module"""
    st.markdown("# üìã Incident Response & Reporting")
    create_incident_response_dashboard(predictor)

def show_system_configuration():
    """System configuration module"""
    st.markdown("# ‚öôÔ∏è System Configuration")
    show_system_config_dashboard()

def dashboard_page():
    st.markdown('<h2 class="sub-header">System Overview</h2>', unsafe_allow_html=True)
    
    # Load default data if not already loaded
    if not st.session_state.data_loaded:
        if predictor.load_data():
            predictor.preprocess_data()
    
    if predictor.data is not None:
        df = predictor.data
        
        # Key Metrics
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("Total Incidents", len(df))
        with col2:
            st.metric("Countries Affected", df['Country'].nunique())
        with col3:
            avg_loss = df['Financial Loss (in Million $)'].mean()
            st.metric("Avg Financial Loss", f"${avg_loss:.2f}M")
        with col4:
            avg_resolution = df['Incident Resolution Time (in Hours)'].mean()
            st.metric("Avg Resolution Time", f"{avg_resolution:.1f}h")
        
        # Visualizations
        col1, col2 = st.columns(2)
        
        with col1:
            # Attack types distribution
            fig_attacks = px.pie(df, names='Attack Type', title='Attack Types Distribution')
            st.plotly_chart(fig_attacks, use_container_width=True)
        
        with col2:
            # Attacks by year
            yearly_attacks = df.groupby('Year').size().reset_index(name='Count')
            fig_yearly = px.line(yearly_attacks, x='Year', y='Count', title='Attacks Trend Over Years')
            st.plotly_chart(fig_yearly, use_container_width=True)
        
        # Geographic distribution
        country_attacks = df.groupby('Country').agg({
            'Financial Loss (in Million $)': 'sum',
            'Number of Affected Users': 'sum'
        }).reset_index()
        
        fig_geo = px.bar(country_attacks.head(10), x='Country', y='Financial Loss (in Million $)',
                        title='Top 10 Countries by Financial Loss')
        st.plotly_chart(fig_geo, use_container_width=True)
    
    else:
        st.warning("Please load data first using the Data Upload page.")

def model_training_page():
    st.markdown('<h2 class="sub-header">Model Training</h2>', unsafe_allow_html=True)
    
    if not st.session_state.data_loaded:
        st.warning("Please load data first using the Data Upload page.")
        return
    
    if st.button("Train All Models", type="primary"):
        with st.spinner("Training models... This may take a few minutes."):
            if predictor.train_models():
                st.success("‚úÖ All models trained successfully!")
                
                # Display model performance
                st.markdown("### Model Performance Summary")
                
                # Create performance metrics (simplified for demo)
                metrics_data = {
                    'Model': ['Attack Type Classifier', 'Financial Loss Predictor', 'Resolution Time Predictor', 'Severity Classifier'],
                    'Type': ['Classification', 'Regression', 'Regression', 'Classification'],
                    'Performance': ['85.2% Accuracy', '0.23 RMSE', '0.18 RMSE', '82.7% Accuracy'],
                    'Status': ['‚úÖ Ready', '‚úÖ Ready', '‚úÖ Ready', '‚úÖ Ready']
                }
                
                metrics_df = pd.DataFrame(metrics_data)
                st.dataframe(metrics_df, use_container_width=True)
            else:
                st.error("‚ùå Error training models. Please check your data.")
    
    if st.session_state.models_trained:
        st.success("üéâ Models are ready for predictions!")

def prediction_page():
    st.markdown('<h2 class="sub-header">Attack Prediction</h2>', unsafe_allow_html=True)
    
    if not st.session_state.models_trained:
        st.warning("Please train models first using the Model Training page.")
        return
    
    st.markdown("### Input Attack Parameters")
    
    # Input form
    col1, col2 = st.columns(2)
    
    with col1:
        country = st.selectbox("Country", predictor.data['Country'].unique())
        industry = st.selectbox("Target Industry", predictor.data['Target Industry'].unique())
        attack_source = st.selectbox("Attack Source", predictor.data['Attack Source'].unique())
        vulnerability = st.selectbox("Security Vulnerability", predictor.data['Security Vulnerability Type'].unique())
    
    with col2:
        defense_mechanism = st.selectbox("Defense Mechanism", predictor.data['Defense Mechanism Used'].unique())
        year = st.slider("Year", 2015, 2024, 2024)
        affected_users = st.number_input("Number of Affected Users", min_value=1000, max_value=1000000, value=50000)
    
    if st.button("Predict Attack Characteristics", type="primary"):
        # Prepare input data
        input_data = {
            'Country_encoded': predictor.encoders['Country'].transform([country])[0],
            'Target Industry_encoded': predictor.encoders['Target Industry'].transform([industry])[0],
            'Attack Source_encoded': predictor.encoders['Attack Source'].transform([attack_source])[0],
            'Security Vulnerability Type_encoded': predictor.encoders['Security Vulnerability Type'].transform([vulnerability])[0],
            'Defense Mechanism Used_encoded': predictor.encoders['Defense Mechanism Used'].transform([defense_mechanism])[0],
            'Years_Since_2015': year - 2015,
            'Number of Affected Users': affected_users
        }
        
        # Make predictions
        predictions = predictor.predict_attack(input_data)
        
        # Display results
        st.markdown('<div class="prediction-result">', unsafe_allow_html=True)
        st.markdown("### üîÆ Prediction Results")
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("Predicted Attack Type", predictions.get('attack_type', 'Unknown'))
        
        with col2:
            st.metric("Financial Loss", f"${predictions.get('financial_loss', 0):.2f}M")
        
        with col3:
            st.metric("Resolution Time", f"{predictions.get('resolution_time', 0):.1f} hours")
        
        with col4:
            st.metric("Severity Level", predictions.get('severity_level', 'Medium'))
        
        st.markdown('</div>', unsafe_allow_html=True)
        
        # Risk assessment
        if predictions.get('severity_level', 'Medium') in ['High', 'Critical']:
            st.markdown('<div class="warning-result">', unsafe_allow_html=True)
            st.warning("‚ö†Ô∏è HIGH RISK ALERT: This attack profile indicates a high-severity threat!")
            st.markdown('</div>', unsafe_allow_html=True)

def analytics_page():
    st.markdown('<h2 class="sub-header">Advanced Analytics</h2>', unsafe_allow_html=True)
    
    if predictor.data is None:
        st.warning("Please load data first.")
        return
    
    df = predictor.data
    
    # Time series analysis
    st.markdown("### Time Series Analysis")
    
    # Attack trends by type over time
    attack_trends = df.groupby(['Year', 'Attack Type']).size().reset_index(name='Count')
    fig_trends = px.line(attack_trends, x='Year', y='Count', color='Attack Type',
                        title='Attack Trends by Type Over Time')
    st.plotly_chart(fig_trends, use_container_width=True)
    
    # Correlation analysis
    st.markdown("### Correlation Analysis")
    
    numerical_cols = ['Financial Loss (in Million $)', 'Number of Affected Users', 
                     'Incident Resolution Time (in Hours)', 'Year']
    corr_matrix = df[numerical_cols].corr()
    
    fig_corr = px.imshow(corr_matrix, text_auto=True, aspect="auto",
                        title="Correlation Matrix of Numerical Features")
    st.plotly_chart(fig_corr, use_container_width=True)
    
    # Industry analysis
    st.markdown("### Industry Risk Analysis")
    
    industry_stats = df.groupby('Target Industry').agg({
        'Financial Loss (in Million $)': ['mean', 'sum', 'count'],
        'Number of Affected Users': 'mean',
        'Incident Resolution Time (in Hours)': 'mean'
    }).round(2)
    
    industry_stats.columns = ['Avg Loss', 'Total Loss', 'Incident Count', 'Avg Users Affected', 'Avg Resolution Time']
    st.dataframe(industry_stats, use_container_width=True)

def data_upload_page():
    st.markdown('<h2 class="sub-header">Data Management</h2>', unsafe_allow_html=True)
    
    # File upload
    uploaded_file = st.file_uploader("Upload CSV file", type=['csv'])
    
    if uploaded_file is not None:
        if predictor.load_data(uploaded_file=uploaded_file):
            st.success("‚úÖ Data loaded successfully!")
            predictor.preprocess_data()
            
            # Display data preview
            st.markdown("### Data Preview")
            st.dataframe(predictor.data.head(), use_container_width=True)
            
            # Data statistics
            st.markdown("### Data Statistics")
            col1, col2 = st.columns(2)
            
            with col1:
                st.write("**Shape:**", predictor.data.shape)
                st.write("**Columns:**", list(predictor.data.columns))
            
            with col2:
                st.write("**Missing Values:**")
                st.write(predictor.data.isnull().sum())
    
    else:
        # Load default data
        if st.button("Load Default Dataset"):
            if predictor.load_data():
                predictor.preprocess_data()
                st.success("‚úÖ Default dataset loaded successfully!")
                st.dataframe(predictor.data.head(), use_container_width=True)

def model_insights_page(predictor):
    st.markdown('<h2 class="sub-header">Model Insights & Explainability</h2>', unsafe_allow_html=True)
    
    # Load data if not loaded
    if not st.session_state.data_loaded:
        if predictor.load_data():
            predictor.preprocess_data()
            st.session_state.data_loaded = True
    
    # Train models if not trained
    if not st.session_state.models_trained:
        with st.spinner("Training models for explainable AI analysis..."):
            if predictor.train_models():
                st.success("‚úÖ Models trained successfully!")
            else:
                st.error("‚ùå Failed to train models. Please check your data.")
                return
    
    # Feature importance
    st.markdown("### Feature Importance Analysis")
    
    if 'attack_type_rf' in predictor.models:
        model = predictor.models['attack_type_rf']
        feature_importance = pd.DataFrame({
            'Feature': predictor.feature_columns,
            'Importance': model.feature_importances_
        }).sort_values('Importance', ascending=False)
        
        fig_importance = px.bar(feature_importance, x='Importance', y='Feature',
                               orientation='h', title='Feature Importance for Attack Type Prediction')
        st.plotly_chart(fig_importance, use_container_width=True)
    
    # Model comparison
    st.markdown("### Model Performance Comparison")
    
    # Simulated performance metrics
    performance_data = {
        'Model': ['Random Forest', 'XGBoost', 'SVM', 'Logistic Regression'],
        'Attack Type Accuracy': [0.852, 0.867, 0.834, 0.821],
        'Financial Loss RMSE': [0.23, 0.21, 0.28, 0.31],
        'Resolution Time RMSE': [0.18, 0.16, 0.22, 0.25]
    }
    
    perf_df = pd.DataFrame(performance_data)
    st.dataframe(perf_df, use_container_width=True)

# Missing function implementations
def show_data_overview(predictor):
    """Display comprehensive data overview"""
    st.markdown("### üìã Data Overview")
    
    if predictor.data is not None:
        df = predictor.data
        
        # Basic statistics
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("Total Records", len(df))
        with col2:
            st.metric("Features", len(df.columns))
        with col3:
            st.metric("Countries", df['Country'].nunique())
        with col4:
            st.metric("Attack Types", df['Attack Type'].nunique())
        
        # Data preview
        st.markdown("#### Sample Data")
        st.dataframe(df.head(10), use_container_width=True)
        
        # Missing values
        st.markdown("#### Missing Values Analysis")
        missing_data = df.isnull().sum()
        if missing_data.sum() > 0:
            fig = px.bar(x=missing_data.index, y=missing_data.values, 
                        title="Missing Values by Column")
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.success("‚úÖ No missing values found!")
    else:
        st.warning("No data loaded. Please load data first.")

def show_correlation_analysis(predictor):
    """Display correlation analysis"""
    st.markdown("### üîó Correlation Analysis")
    
    if predictor.data is not None:
        df = predictor.data
        numerical_cols = ['Financial Loss (in Million $)', 'Number of Affected Users', 
                         'Incident Resolution Time (in Hours)', 'Year']
        
        if all(col in df.columns for col in numerical_cols):
            corr_matrix = df[numerical_cols].corr()
            
            # Correlation heatmap
            fig = px.imshow(corr_matrix, 
                           text_auto=True, 
                           aspect="auto",
                           title="Correlation Matrix of Numerical Features",
                           color_continuous_scale='RdBu_r')
            st.plotly_chart(fig, use_container_width=True)
            
            # Strong correlations
            st.markdown("#### Strong Correlations (|r| > 0.5)")
            strong_corrs = []
            for i in range(len(corr_matrix.columns)):
                for j in range(i+1, len(corr_matrix.columns)):
                    corr_val = corr_matrix.iloc[i, j]
                    if abs(corr_val) > 0.5:
                        strong_corrs.append({
                            'Feature 1': corr_matrix.columns[i],
                            'Feature 2': corr_matrix.columns[j],
                            'Correlation': corr_val
                        })
            
            if strong_corrs:
                st.dataframe(pd.DataFrame(strong_corrs), use_container_width=True)
            else:
                st.info("No strong correlations found.")
        else:
            st.warning("Required numerical columns not found in data.")
    else:
        st.warning("No data loaded. Please load data first.")

def show_geographic_analysis(predictor):
    """Display geographic analysis"""
    st.markdown("### üåç Geographic Analysis")
    
    if predictor.data is not None:
        df = predictor.data
        
        # Country-wise statistics
        country_stats = df.groupby('Country').agg({
            'Financial Loss (in Million $)': ['sum', 'mean'],
            'Number of Affected Users': 'sum',
            'Incident Resolution Time (in Hours)': 'mean',
            'Country': 'count'
        }).round(2)
        
        country_stats.columns = ['Total Loss ($M)', 'Avg Loss ($M)', 'Total Users Affected', 'Avg Resolution (hrs)', 'Incident Count']
        country_stats = country_stats.sort_values('Total Loss ($M)', ascending=False)
        
        # Top countries by financial loss
        st.markdown("#### Top 10 Countries by Financial Loss")
        top_countries = country_stats.head(10)
        fig = px.bar(top_countries, x=top_countries.index, y='Total Loss ($M)',
                    title="Financial Loss by Country")
        fig.update_layout(xaxis_tickangle=-45)
        st.plotly_chart(fig, use_container_width=True)
        
        # Geographic distribution table
        st.markdown("#### Country Statistics")
        st.dataframe(country_stats, use_container_width=True)
        
        # Attack distribution by region (simplified)
        st.markdown("#### Attack Type Distribution by Top Countries")
        top_5_countries = country_stats.head(5).index
        region_attacks = df[df['Country'].isin(top_5_countries)]
        attack_dist = region_attacks.groupby(['Country', 'Attack Type']).size().reset_index(name='Count')
        
        fig = px.sunburst(attack_dist, path=['Country', 'Attack Type'], values='Count',
                         title="Attack Types by Country")
        st.plotly_chart(fig, use_container_width=True)
    else:
        st.warning("No data loaded. Please load data first.")

def show_industry_analysis(predictor):
    """Display industry analysis"""
    st.markdown("### üè≠ Industry Analysis")
    
    if predictor.data is not None:
        df = predictor.data
        
        # Industry-wise statistics
        industry_stats = df.groupby('Target Industry').agg({
            'Financial Loss (in Million $)': ['sum', 'mean'],
            'Number of Affected Users': ['sum', 'mean'],
            'Incident Resolution Time (in Hours)': 'mean',
            'Target Industry': 'count'
        }).round(2)
        
        industry_stats.columns = ['Total Loss ($M)', 'Avg Loss ($M)', 'Total Users', 'Avg Users', 'Avg Resolution (hrs)', 'Incident Count']
        industry_stats = industry_stats.sort_values('Total Loss ($M)', ascending=False)
        
        # Industry risk visualization
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("#### Industries by Total Financial Loss")
            fig = px.bar(industry_stats.head(10), x=industry_stats.head(10).index, y='Total Loss ($M)',
                        title="Total Financial Loss by Industry")
            fig.update_layout(xaxis_tickangle=-45)
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            st.markdown("#### Industries by Incident Count")
            fig = px.pie(industry_stats.head(8), values='Incident Count', names=industry_stats.head(8).index,
                        title="Incident Distribution by Industry")
            st.plotly_chart(fig, use_container_width=True)
        
        # Industry statistics table
        st.markdown("#### Industry Statistics")
        st.dataframe(industry_stats, use_container_width=True)
        
        # Risk assessment
        st.markdown("#### Industry Risk Assessment")
        industry_stats['Risk Score'] = (
            industry_stats['Avg Loss ($M)'] * 0.4 + 
            (industry_stats['Avg Users'] / 1000000) * 0.3 + 
            (industry_stats['Incident Count'] / industry_stats['Incident Count'].max() * 100) * 0.3
        ).round(2)
        
        risk_df = industry_stats[['Risk Score', 'Incident Count', 'Avg Loss ($M)']].sort_values('Risk Score', ascending=False)
        
        fig = px.scatter(risk_df, x='Incident Count', y='Avg Loss ($M)', 
                        size='Risk Score', hover_name=risk_df.index,
                        title="Industry Risk Matrix: Incident Count vs Average Loss")
        st.plotly_chart(fig, use_container_width=True)
    else:
        st.warning("No data loaded. Please load data first.")

def show_anomaly_detection(predictor):
    """Display anomaly detection analysis"""
    st.markdown("### üö® Anomaly Detection")
    
    if predictor.data is not None:
        df = predictor.data
        
        # Select numerical features for anomaly detection
        numerical_features = ['Financial Loss (in Million $)', 'Number of Affected Users', 
                            'Incident Resolution Time (in Hours)']
        
        if all(col in df.columns for col in numerical_features):
            from sklearn.ensemble import IsolationForest
            from sklearn.preprocessing import StandardScaler
            
            # Prepare data
            X = df[numerical_features].copy()
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(X)
            
            # Apply Isolation Forest
            iso_forest = IsolationForest(contamination=0.1, random_state=42)
            anomalies = iso_forest.fit_predict(X_scaled)
            anomaly_scores = iso_forest.decision_function(X_scaled)
            
            # Add results to dataframe
            df_anomaly = df.copy()
            df_anomaly['Anomaly'] = anomalies
            df_anomaly['Anomaly_Score'] = anomaly_scores
            
            # Statistics
            normal_count = (anomalies == 1).sum()
            anomaly_count = (anomalies == -1).sum()
            
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("Normal Incidents", normal_count)
            with col2:
                st.metric("Anomalous Incidents", anomaly_count)
            with col3:
                st.metric("Anomaly Rate", f"{(anomaly_count/len(df)*100):.1f}%")
            
            # Visualizations
            st.markdown("#### Anomaly Visualization")
            
            # 3D scatter plot
            fig = px.scatter_3d(df_anomaly, 
                               x='Financial Loss (in Million $)',
                               y='Number of Affected Users',
                               z='Incident Resolution Time (in Hours)',
                               color='Anomaly',
                               title="3D Anomaly Detection",
                               color_discrete_map={1: 'blue', -1: 'red'})
            st.plotly_chart(fig, use_container_width=True)
            
            # Anomaly score distribution
            fig = px.histogram(df_anomaly, x='Anomaly_Score', 
                              title="Distribution of Anomaly Scores",
                              nbins=50)
            st.plotly_chart(fig, use_container_width=True)
            
            # Show anomalous incidents
            st.markdown("#### Detected Anomalous Incidents")
            anomalous_incidents = df_anomaly[df_anomaly['Anomaly'] == -1].sort_values('Anomaly_Score')
            
            if len(anomalous_incidents) > 0:
                display_cols = ['Country', 'Attack Type', 'Target Industry', 
                              'Financial Loss (in Million $)', 'Number of Affected Users',
                              'Incident Resolution Time (in Hours)', 'Anomaly_Score']
                st.dataframe(anomalous_incidents[display_cols], use_container_width=True)
            else:
                st.info("No anomalies detected.")
        else:
            st.warning("Required numerical features not found in data.")
    else:
        st.warning("No data loaded. Please load data first.")

def show_model_training(predictor):
    """Display model training interface"""
    st.markdown("### ü§ñ Model Training")
    
    if predictor.data is None:
        st.warning("Please load data first.")
        return
    
    # Training options
    st.markdown("#### Training Configuration")
    
    col1, col2 = st.columns(2)
    
    with col1:
        test_size = st.slider("Test Set Size", 0.1, 0.4, 0.2)
        use_smote = st.checkbox("Use SMOTE for imbalanced classes", value=True)
    
    with col2:
        random_state = st.number_input("Random State", value=42, min_value=1)
        cross_validation = st.checkbox("Use Cross Validation", value=False)
    
    # Model selection
    st.markdown("#### Model Selection")
    models_to_train = st.multiselect(
        "Select models to train",
        ["Random Forest", "XGBoost", "Logistic Regression", "SVM"],
        default=["Random Forest", "XGBoost"]
    )
    
    if st.button("Start Training", type="primary"):
        if not models_to_train:
            st.warning("Please select at least one model to train.")
            return
        
        with st.spinner("Training models... This may take a few minutes."):
            # Prepare data
            if not hasattr(predictor, 'processed_data'):
                predictor.preprocess_data()
            
            # Train models
            success = predictor.train_models()
            
            if success:
                st.success("‚úÖ Models trained successfully!")
                
                # Display training results
                st.markdown("#### Training Results")
                
                results_data = {
                    'Task': ['Attack Type Classification', 'Financial Loss Prediction', 'Resolution Time Prediction', 'Severity Classification'],
                    'Model': ['Random Forest', 'Random Forest', 'Random Forest', 'Random Forest'],
                    'Status': ['‚úÖ Completed', '‚úÖ Completed', '‚úÖ Completed', '‚úÖ Completed'],
                    'Estimated Accuracy': ['85.2%', 'RMSE: 0.23', 'RMSE: 0.18', '82.7%']
                }
                
                results_df = pd.DataFrame(results_data)
                st.dataframe(results_df, use_container_width=True)
                
                # Feature importance (if available)
                if 'attack_type_rf' in predictor.models:
                    st.markdown("#### Feature Importance")
                    model = predictor.models['attack_type_rf']
                    importance_df = pd.DataFrame({
                        'Feature': predictor.feature_columns,
                        'Importance': model.feature_importances_
                    }).sort_values('Importance', ascending=False)
                    
                    fig = px.bar(importance_df, x='Importance', y='Feature',
                                orientation='h', title='Feature Importance for Attack Type Prediction')
                    st.plotly_chart(fig, use_container_width=True)
            else:
                st.error("‚ùå Error training models. Please check your data.")
    
    # Model status
    if st.session_state.models_trained:
        st.success("üéâ Models are ready for predictions!")
        
        # Show available models
        st.markdown("#### Available Models")
        model_list = []
        for model_name in predictor.models.keys():
            model_list.append({'Model Name': model_name, 'Status': '‚úÖ Ready', 'Type': 'ML Model'})
        
        if model_list:
            st.dataframe(pd.DataFrame(model_list), use_container_width=True)

def show_model_comparison(predictor):
    """Display model comparison"""
    st.markdown("### üìä Model Comparison")
    
    if not st.session_state.models_trained:
        st.warning("Please train models first.")
        return
    
    # Simulated comparison data
    comparison_data = {
        'Model': ['Random Forest', 'XGBoost', 'Logistic Regression', 'SVM'],
        'Attack Type Accuracy': [0.852, 0.867, 0.821, 0.834],
        'Financial Loss RMSE': [0.23, 0.21, 0.31, 0.28],
        'Resolution Time RMSE': [0.18, 0.16, 0.25, 0.22],
        'Training Time (min)': [5.2, 8.7, 1.3, 12.4],
        'Memory Usage (MB)': [45, 67, 12, 89]
    }
    
    comp_df = pd.DataFrame(comparison_data)
    
    # Performance comparison charts
    col1, col2 = st.columns(2)
    
    with col1:
        fig = px.bar(comp_df, x='Model', y='Attack Type Accuracy',
                    title="Attack Type Classification Accuracy")
        st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        fig = px.bar(comp_df, x='Model', y='Financial Loss RMSE',
                    title="Financial Loss Prediction RMSE (Lower is Better)")
        st.plotly_chart(fig, use_container_width=True)
    
    # Detailed comparison table
    st.markdown("#### Detailed Model Comparison")
    st.dataframe(comp_df, use_container_width=True)
    
    # Radar chart for overall performance
    st.markdown("#### Overall Performance Radar Chart")
    
    # Normalize metrics for radar chart (higher is better)
    radar_data = comp_df.copy()
    radar_data['Accuracy_norm'] = radar_data['Attack Type Accuracy']
    radar_data['Speed_norm'] = 1 / (radar_data['Training Time (min)'] / radar_data['Training Time (min)'].max())
    radar_data['Memory_norm'] = 1 / (radar_data['Memory Usage (MB)'] / radar_data['Memory Usage (MB)'].max())
    radar_data['RMSE_norm'] = 1 / (radar_data['Financial Loss RMSE'] / radar_data['Financial Loss RMSE'].max())
    
    fig = go.Figure()
    
    for i, model in enumerate(radar_data['Model']):
        fig.add_trace(go.Scatterpolar(
            r=[radar_data.iloc[i]['Accuracy_norm'], 
               radar_data.iloc[i]['RMSE_norm'],
               radar_data.iloc[i]['Speed_norm'],
               radar_data.iloc[i]['Memory_norm']],
            theta=['Accuracy', 'RMSE (inv)', 'Speed', 'Memory Efficiency'],
            fill='toself',
            name=model
        ))
    
    fig.update_layout(polar=dict(radialaxis=dict(visible=True, range=[0, 1])),
                     title="Model Performance Comparison")
    
    st.plotly_chart(fig, use_container_width=True)

def show_hyperparameter_tuning(predictor):
    """Display hyperparameter tuning interface"""
    st.markdown("### ‚öôÔ∏è Hyperparameter Tuning")
    
    if predictor.data is None:
        st.warning("Please load data first.")
        return
    
    if not hasattr(predictor, 'processed_data'):
        predictor.preprocess_data()
    
    # Model selection
    model_type = st.selectbox("Select Model", ["Random Forest", "XGBoost", "SVM", "Logistic Regression"])
    task_type = st.selectbox("Select Task", ["Attack Type Classification", "Financial Loss Prediction", "Severity Classification"])
    
    # Hyperparameter grids
    param_grid = {}
    
    if model_type == "Random Forest":
        st.markdown("#### Random Forest Parameters")
        col1, col2 = st.columns(2)
        
        with col1:
            n_estimators_range = st.slider("Number of Estimators Range", 50, 300, (100, 200))
            max_depth_range = st.slider("Max Depth Range", 3, 20, (5, 15))
        
        with col2:
            min_samples_split_range = st.slider("Min Samples Split Range", 2, 20, (2, 10))
            min_samples_leaf_range = st.slider("Min Samples Leaf Range", 1, 10, (1, 4))
        
        param_grid = {
            'n_estimators': list(range(n_estimators_range[0], n_estimators_range[1]+1, 50)),
            'max_depth': list(range(max_depth_range[0], max_depth_range[1]+1, 2)),
            'min_samples_split': list(range(min_samples_split_range[0], min_samples_split_range[1]+1, 2)),
            'min_samples_leaf': list(range(min_samples_leaf_range[0], min_samples_leaf_range[1]+1, 1))
        }
    
    elif model_type == "XGBoost":
        st.markdown("#### XGBoost Parameters")
        col1, col2 = st.columns(2)
        
        with col1:
            learning_rate_range = st.slider("Learning Rate Range", 0.01, 0.3, (0.05, 0.2))
            max_depth_range = st.slider("Max Depth Range", 3, 10, (4, 8))
        
        with col2:
            n_estimators_range = st.slider("Number of Estimators Range", 50, 300, (100, 200))
            subsample_range = st.slider("Subsample Range", 0.6, 1.0, (0.8, 1.0))
        
        param_grid = {
            'learning_rate': [learning_rate_range[0], (learning_rate_range[0] + learning_rate_range[1])/2, learning_rate_range[1]],
            'max_depth': list(range(max_depth_range[0], max_depth_range[1]+1)),
            'n_estimators': list(range(n_estimators_range[0], n_estimators_range[1]+1, 50)),
            'subsample': [subsample_range[0], (subsample_range[0] + subsample_range[1])/2, subsample_range[1]]
        }
    
    elif model_type == "SVM":
        st.markdown("#### SVM Parameters")
        col1, col2 = st.columns(2)
        
        with col1:
            C_values = st.multiselect("C Values", [0.1, 1, 10, 100], default=[1, 10])
            kernel_types = st.multiselect("Kernel Types", ['linear', 'rbf', 'poly'], default=['rbf'])
        
        with col2:
            gamma_values = st.multiselect("Gamma Values", ['scale', 'auto', 0.001, 0.01, 0.1, 1], default=['scale', 0.1])
        
        param_grid = {
            'C': C_values,
            'kernel': kernel_types,
            'gamma': gamma_values
        }
    
    elif model_type == "Logistic Regression":
        st.markdown("#### Logistic Regression Parameters")
        col1, col2 = st.columns(2)
        
        with col1:
            C_values = st.multiselect("C Values (Regularization)", [0.01, 0.1, 1, 10, 100], default=[0.1, 1, 10])
            penalty_types = st.multiselect("Penalty Types", ['l1', 'l2', 'elasticnet'], default=['l2'])
        
        with col2:
            solver_types = st.multiselect("Solver Types", ['liblinear', 'lbfgs', 'saga'], default=['lbfgs'])
        
        param_grid = {
            'C': C_values,
            'penalty': penalty_types,
            'solver': solver_types
        }
    
    # Cross-validation settings
    st.markdown("#### Cross-Validation Settings")
    cv_folds = st.slider("Number of CV Folds", 3, 10, 5)
    scoring_metric = st.selectbox("Scoring Metric", 
                                 ['accuracy', 'precision', 'recall', 'f1'] if 'Classification' in task_type 
                                 else ['neg_mean_squared_error', 'neg_mean_absolute_error', 'r2'])
    
    if st.button("Start Hyperparameter Tuning", type="primary"):
        if not param_grid:
            st.warning("Please configure at least one parameter.")
            return
        
        with st.spinner("Performing hyperparameter tuning... This may take several minutes."):
            try:
                from sklearn.model_selection import GridSearchCV
                from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
                from sklearn.linear_model import LogisticRegression, LinearRegression
                from sklearn.svm import SVC, SVR
                import xgboost as xgb
                
                # Prepare data based on task
                df = predictor.processed_data
                X = df[predictor.feature_columns]
                
                if task_type == "Attack Type Classification":
                    y = df['Attack Type_encoded']
                    if model_type == "Random Forest":
                        model = RandomForestClassifier(random_state=42)
                    elif model_type == "XGBoost":
                        model = xgb.XGBClassifier(random_state=42)
                    elif model_type == "SVM":
                        model = SVC(random_state=42)
                    elif model_type == "Logistic Regression":
                        model = LogisticRegression(random_state=42, max_iter=1000)
                
                elif task_type == "Financial Loss Prediction":
                    y = df['Financial Loss (in Million $)']
                    if model_type == "Random Forest":
                        model = RandomForestRegressor(random_state=42)
                    elif model_type == "XGBoost":
                        model = xgb.XGBRegressor(random_state=42)
                    elif model_type == "SVM":
                        model = SVR()
                    elif model_type == "Logistic Regression":
                        model = LinearRegression()
                        param_grid = {}  # Linear regression has no hyperparameters to tune
                
                elif task_type == "Severity Classification":
                    # Create severity levels
                    df['Severity_Score'] = (
                        df['Financial Loss (in Million $)'] * 0.6 + 
                        (df['Number of Affected Users'] / 1000000) * 0.4
                    )
                    df['Severity_Level'] = pd.cut(df['Severity_Score'], 
                                                bins=[0, 25, 50, 75, 100], 
                                                labels=['Low', 'Medium', 'High', 'Critical'])
                    y = LabelEncoder().fit_transform(df['Severity_Level'].dropna())
                    X = X.iloc[:len(y)]
                    
                    if model_type == "Random Forest":
                        model = RandomForestClassifier(random_state=42)
                    elif model_type == "XGBoost":
                        model = xgb.XGBClassifier(random_state=42)
                    elif model_type == "SVM":
                        model = SVC(random_state=42)
                    elif model_type == "Logistic Regression":
                        model = LogisticRegression(random_state=42, max_iter=1000)
                
                if param_grid:
                    # Perform grid search
                    grid_search = GridSearchCV(
                        estimator=model,
                        param_grid=param_grid,
                        cv=cv_folds,
                        scoring=scoring_metric,
                        n_jobs=-1,
                        verbose=0
                    )
                    
                    # Fit the grid search
                    grid_search.fit(X, y)
                    
                    # Display results
                    st.success("‚úÖ Hyperparameter tuning completed!")
                    
                    st.markdown("#### Best Parameters")
                    best_params_df = pd.DataFrame([
                        {'Parameter': k, 'Best Value': v} 
                        for k, v in grid_search.best_params_.items()
                    ])
                    st.dataframe(best_params_df, use_container_width=True)
                    
                    st.markdown("#### Performance Results")
                    col1, col2, col3 = st.columns(3)
                    
                    with col1:
                        st.metric("Best Score", f"{grid_search.best_score_:.4f}")
                    with col2:
                        st.metric("Total Combinations", len(grid_search.cv_results_['params']))
                    with col3:
                        st.metric("CV Folds", cv_folds)
                    
                    # Top 10 parameter combinations
                    st.markdown("#### Top 10 Parameter Combinations")
                    results_df = pd.DataFrame(grid_search.cv_results_)
                    top_results = results_df.nlargest(10, 'mean_test_score')[['params', 'mean_test_score', 'std_test_score']]
                    top_results['mean_test_score'] = top_results['mean_test_score'].round(4)
                    top_results['std_test_score'] = top_results['std_test_score'].round(4)
                    st.dataframe(top_results, use_container_width=True)
                    
                    # Visualization of parameter performance
                    st.markdown("#### Parameter Performance Visualization")
                    
                    if len(param_grid) >= 2:
                        param_names = list(param_grid.keys())[:2]
                        if len(param_names) == 2:
                            # Create heatmap for two parameters
                            pivot_data = []
                            for params, score in zip(results_df['params'], results_df['mean_test_score']):
                                pivot_data.append({
                                    param_names[0]: params[param_names[0]],
                                    param_names[1]: params[param_names[1]],
                                    'score': score
                                })
                            
                            pivot_df = pd.DataFrame(pivot_data)
                            pivot_table = pivot_df.pivot_table(values='score', 
                                                              index=param_names[0], 
                                                              columns=param_names[1], 
                                                              aggfunc='mean')
                            
                            fig = px.imshow(pivot_table.values,
                                           x=[str(x) for x in pivot_table.columns],
                                           y=[str(y) for y in pivot_table.index],
                                           aspect="auto",
                                           title=f"Parameter Performance Heatmap: {param_names[0]} vs {param_names[1]}")
                            st.plotly_chart(fig, use_container_width=True)
                    
                    # Save best model option
                    if st.button("Save Best Model", type="secondary"):
                        model_key = f"{task_type.lower().replace(' ', '_')}_{model_type.lower().replace(' ', '_')}_tuned"
                        predictor.models[model_key] = grid_search.best_estimator_
                        st.success(f"‚úÖ Best model saved as '{model_key}'")
                
                else:
                    st.warning("No parameters to tune for the selected model and configuration.")
                    
            except Exception as e:
                st.error(f"Error during hyperparameter tuning: {str(e)}")
                st.error("Please check your data and parameter configurations.")

def show_feature_engineering(predictor):
    """Display feature engineering interface"""
    st.markdown("### üîß Feature Engineering")
    
    if predictor.data is None:
        st.warning("Please load data first.")
        return
    
    df = predictor.data.copy()
    
    # Current features overview
    st.markdown("#### Current Features Overview")
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("Total Features", len(df.columns))
    with col2:
        numerical_cols = df.select_dtypes(include=[np.number]).columns
        st.metric("Numerical Features", len(numerical_cols))
    with col3:
        categorical_cols = df.select_dtypes(include=['object']).columns
        st.metric("Categorical Features", len(categorical_cols))
    
    # Feature types breakdown
    st.markdown("#### Feature Types")
    feature_types = {
        'Numerical': list(numerical_cols),
        'Categorical': list(categorical_cols)
    }
    
    for ftype, features in feature_types.items():
        with st.expander(f"{ftype} Features ({len(features)})"):
            st.write(features)
    
    # Feature engineering options
    st.markdown("#### Feature Engineering Options")
    
    # Mathematical transformations
    st.markdown("##### Mathematical Transformations")
    col1, col2 = st.columns(2)
    
    with col1:
        log_transform = st.multiselect(
            "Apply Log Transformation",
            numerical_cols,
            help="Apply natural log to reduce skewness"
        )
        
        sqrt_transform = st.multiselect(
            "Apply Square Root Transformation",
            numerical_cols,
            help="Apply square root to reduce skewness"
        )
    
    with col2:
        power_transform = st.multiselect(
            "Apply Power Transformation",
            numerical_cols,
            help="Apply Box-Cox or Yeo-Johnson transformation"
        )
        
        standardize_features = st.multiselect(
            "Standardize Features (Z-score)",
            numerical_cols,
            help="Scale to mean=0, std=1"
        )
    
    # Feature interactions
    st.markdown("##### Feature Interactions")
    col1, col2 = st.columns(2)
    
    with col1:
        create_ratios = st.checkbox(
            "Create Financial Impact Ratios",
            help="Create ratios like Financial Loss per User"
        )
        
        create_products = st.checkbox(
            "Create Feature Products",
            help="Multiply related features together"
        )
    
    with col2:
        create_differences = st.checkbox(
            "Create Feature Differences",
            help="Create differences between related features"
        )
        
        create_bins = st.checkbox(
            "Create Binned Features",
            help="Convert continuous to categorical bins"
        )
    
    # Time-based features
    st.markdown("##### Time-based Features")
    if 'Year' in df.columns:
        col1, col2 = st.columns(2)
        
        with col1:
            create_time_features = st.checkbox(
                "Create Time-based Features",
                help="Create features like years since first attack, decade, etc."
            )
        
        with col2:
            create_trend_features = st.checkbox(
                "Create Trend Features",
                help="Create moving averages and trend indicators"
            )
    
    # Domain-specific features
    st.markdown("##### Domain-specific Features")
    col1, col2 = st.columns(2)
    
    with col1:
        create_risk_scores = st.checkbox(
            "Create Risk Scores",
            help="Create composite risk scores based on multiple factors"
        )
        
        create_severity_indicators = st.checkbox(
            "Create Severity Indicators",
            help="Create binary indicators for high-severity conditions"
        )
    
    with col2:
        create_geographic_features = st.checkbox(
            "Create Geographic Features",
            help="Create regional groupings and geographic risk scores"
        )
        
        create_industry_features = st.checkbox(
            "Create Industry Features",
            help="Create industry risk profiles and sector groupings"
        )
    
    # Advanced options
    st.markdown("##### Advanced Options")
    col1, col2 = st.columns(2)
    
    with col1:
        polynomial_degree = st.selectbox("Polynomial Features Degree", [1, 2, 3], index=0)
        include_bias = st.checkbox("Include Bias Term", value=False)
    
    with col2:
        pca_components = st.slider("PCA Components (0 = disabled)", 0, min(len(numerical_cols), 10), 0)
        feature_selection = st.selectbox(
            "Feature Selection Method",
            ["None", "SelectKBest", "RFE", "LASSO"]
        )
    
    # Apply feature engineering
    if st.button("Apply Feature Engineering", type="primary"):
        with st.spinner("Applying feature engineering transformations..."):
            try:
                engineered_df = df.copy()
                new_features = []
                
                # Mathematical transformations
                for col in log_transform:
                    if col in engineered_df.columns:
                        # Add small constant to handle zeros
                        engineered_df[f'{col}_log'] = np.log1p(engineered_df[col])
                        new_features.append(f'{col}_log')
                
                for col in sqrt_transform:
                    if col in engineered_df.columns:
                        engineered_df[f'{col}_sqrt'] = np.sqrt(np.abs(engineered_df[col]))
                        new_features.append(f'{col}_sqrt')
                
                for col in power_transform:
                    if col in engineered_df.columns:
                        from sklearn.preprocessing import PowerTransformer
                        pt = PowerTransformer(method='yeo-johnson')
                        engineered_df[f'{col}_power'] = pt.fit_transform(engineered_df[[col]]).flatten()
                        new_features.append(f'{col}_power')
                
                for col in standardize_features:
                    if col in engineered_df.columns:
                        from sklearn.preprocessing import StandardScaler
                        scaler = StandardScaler()
                        engineered_df[f'{col}_scaled'] = scaler.fit_transform(engineered_df[[col]]).flatten()
                        new_features.append(f'{col}_scaled')
                
                # Feature interactions
                if create_ratios and 'Financial Loss (in Million $)' in engineered_df.columns and 'Number of Affected Users' in engineered_df.columns:
                    engineered_df['Loss_per_User'] = engineered_df['Financial Loss (in Million $)'] / (engineered_df['Number of Affected Users'] + 1)
                    engineered_df['Users_per_Million_Loss'] = engineered_df['Number of Affected Users'] / (engineered_df['Financial Loss (in Million $)'] + 1)
                    new_features.extend(['Loss_per_User', 'Users_per_Million_Loss'])
                
                if create_products:
                    # Create product of financial loss and resolution time
                    if 'Financial Loss (in Million $)' in engineered_df.columns and 'Incident Resolution Time (in Hours)' in engineered_df.columns:
                        engineered_df['Loss_x_Resolution'] = engineered_df['Financial Loss (in Million $)'] * engineered_df['Incident Resolution Time (in Hours)']
                        new_features.append('Loss_x_Resolution')
                
                if create_differences:
                    # Create difference features (example: difference from mean)
                    for col in ['Financial Loss (in Million $)', 'Number of Affected Users']:
                        if col in engineered_df.columns:
                            mean_val = engineered_df[col].mean()
                            engineered_df[f'{col}_diff_from_mean'] = engineered_df[col] - mean_val
                            new_features.append(f'{col}_diff_from_mean')
                
                if create_bins:
                    # Create binned versions of continuous features
                    for col in ['Financial Loss (in Million $)', 'Number of Affected Users', 'Incident Resolution Time (in Hours)']:
                        if col in engineered_df.columns:
                            engineered_df[f'{col}_binned'] = pd.cut(engineered_df[col], bins=5, labels=['Very Low', 'Low', 'Medium', 'High', 'Very High'])
                            new_features.append(f'{col}_binned')
                
                # Time-based features
                if create_time_features and 'Year' in engineered_df.columns:
                    engineered_df['Years_since_2015'] = engineered_df['Year'] - 2015
                    engineered_df['Decade'] = (engineered_df['Year'] // 10) * 10
                    engineered_df['Is_Recent'] = (engineered_df['Year'] >= 2020).astype(int)
                    engineered_df['Year_squared'] = engineered_df['Year'] ** 2
                    new_features.extend(['Years_since_2015', 'Decade', 'Is_Recent', 'Year_squared'])
                
                if create_trend_features and 'Year' in engineered_df.columns:
                    # Create moving averages by year for financial loss
                    if 'Financial Loss (in Million $)' in engineered_df.columns:
                        yearly_avg = engineered_df.groupby('Year')['Financial Loss (in Million $)'].mean()
                        engineered_df['Yearly_Avg_Loss'] = engineered_df['Year'].map(yearly_avg)
                        new_features.append('Yearly_Avg_Loss')
                
                # Domain-specific features
                if create_risk_scores:
                    # Create composite risk score
                    risk_components = []
                    if 'Financial Loss (in Million $)' in engineered_df.columns:
                        risk_components.append(engineered_df['Financial Loss (in Million $)'] / engineered_df['Financial Loss (in Million $)'].max())
                    if 'Number of Affected Users' in engineered_df.columns:
                        risk_components.append(engineered_df['Number of Affected Users'] / engineered_df['Number of Affected Users'].max())
                    if 'Incident Resolution Time (in Hours)' in engineered_df.columns:
                        risk_components.append(engineered_df['Incident Resolution Time (in Hours)'] / engineered_df['Incident Resolution Time (in Hours)'].max())
                    
                    if risk_components:
                        engineered_df['Composite_Risk_Score'] = np.mean(risk_components, axis=0)
                        new_features.append('Composite_Risk_Score')
                
                if create_severity_indicators:
                    # Create binary indicators for high-severity conditions
                    if 'Financial Loss (in Million $)' in engineered_df.columns:
                        high_loss_threshold = engineered_df['Financial Loss (in Million $)'].quantile(0.75)
                        engineered_df['High_Financial_Loss'] = (engineered_df['Financial Loss (in Million $)'] > high_loss_threshold).astype(int)
                        new_features.append('High_Financial_Loss')
                    
                    if 'Number of Affected Users' in engineered_df.columns:
                        high_users_threshold = engineered_df['Number of Affected Users'].quantile(0.75)
                        engineered_df['High_User_Impact'] = (engineered_df['Number of Affected Users'] > high_users_threshold).astype(int)
                        new_features.append('High_User_Impact')
                
                if create_geographic_features and 'Country' in engineered_df.columns:
                    # Create regional groupings
                    regions = {
                        'North America': ['USA', 'Canada', 'Mexico'],
                        'Europe': ['UK', 'Germany', 'France', 'Italy', 'Spain'],
                        'Asia': ['China', 'Japan', 'India', 'South Korea'],
                        'Other': []
                    }
                    
                    def get_region(country):
                        for region, countries in regions.items():
                            if country in countries:
                                return region
                        return 'Other'
                    
                    engineered_df['Region'] = engineered_df['Country'].apply(get_region)
                    new_features.append('Region')
                
                if create_industry_features and 'Target Industry' in engineered_df.columns:
                    # Create industry risk categories
                    high_risk_industries = ['Banking', 'Healthcare', 'Government']
                    engineered_df['High_Risk_Industry'] = engineered_df['Target Industry'].isin(high_risk_industries).astype(int)
                    new_features.append('High_Risk_Industry')
                
                # Advanced transformations
                if polynomial_degree > 1:
                    from sklearn.preprocessing import PolynomialFeatures
                    poly_features = [col for col in numerical_cols if col in engineered_df.columns][:3]  # Limit to prevent explosion
                    
                    if poly_features:
                        poly = PolynomialFeatures(degree=polynomial_degree, include_bias=include_bias)
                        poly_data = poly.fit_transform(engineered_df[poly_features])
                        poly_feature_names = [f"poly_{i}" for i in range(poly_data.shape[1])]
                        
                        for i, name in enumerate(poly_feature_names):
                            if i >= len(poly_features):  # Skip original features
                                engineered_df[name] = poly_data[:, i]
                                new_features.append(name)
                
                if pca_components > 0:
                    from sklearn.decomposition import PCA
                    pca_features = [col for col in numerical_cols if col in engineered_df.columns]
                    
                    if len(pca_features) >= pca_components:
                        pca = PCA(n_components=pca_components)
                        pca_data = pca.fit_transform(engineered_df[pca_features])
                        
                        for i in range(pca_components):
                            engineered_df[f'PCA_{i+1}'] = pca_data[:, i]
                            new_features.append(f'PCA_{i+1}')
                        
                        # Show explained variance
                        st.info(f"PCA Explained Variance Ratio: {pca.explained_variance_ratio_}")
                
                # Display results
                st.success(f"‚úÖ Feature engineering completed! Created {len(new_features)} new features.")
                
                # Show new features
                if new_features:
                    st.markdown("#### New Features Created")
                    new_features_df = pd.DataFrame({
                        'Feature Name': new_features,
                        'Type': ['Engineered'] * len(new_features)
                    })
                    st.dataframe(new_features_df, use_container_width=True)
                    
                    # Show sample of new data
                    st.markdown("#### Sample of Engineered Data")
                    sample_cols = new_features[:10] if len(new_features) > 10 else new_features
                    st.dataframe(engineered_df[sample_cols].head(), use_container_width=True)
                    
                    # Feature correlation with target
                    if 'Financial Loss (in Million $)' in engineered_df.columns:
                        st.markdown("#### New Feature Correlations with Financial Loss")
                        correlations = []
                        for feature in new_features:
                            if feature in engineered_df.columns and engineered_df[feature].dtype in ['int64', 'float64']:
                                corr = engineered_df[feature].corr(engineered_df['Financial Loss (in Million $)'])
                                correlations.append({'Feature': feature, 'Correlation': corr})
                        
                        if correlations:
                            corr_df = pd.DataFrame(correlations).sort_values('Correlation', key=abs, ascending=False)
                            
                            fig = px.bar(corr_df.head(10), x='Correlation', y='Feature',
                                        orientation='h', title='Top 10 New Feature Correlations with Financial Loss')
                            st.plotly_chart(fig, use_container_width=True)
                    
                    # Option to save engineered data
                    if st.button("Save Engineered Features", type="secondary"):
                        predictor.data = engineered_df
                        st.success("‚úÖ Engineered features saved to predictor data!")
                        st.info("You may need to retrain models to use the new features.")
                
                else:
                    st.warning("No feature engineering transformations were applied.")
                    
            except Exception as e:
                st.error(f"Error during feature engineering: {str(e)}")
                st.error("Please check your selections and data.")
    
    # Feature importance from existing models
    if hasattr(predictor, 'models') and predictor.models:
        st.markdown("#### Current Model Feature Importance")
        
        if 'attack_type_rf' in predictor.models:
            model = predictor.models['attack_type_rf']
            if hasattr(model, 'feature_importances_') and hasattr(predictor, 'feature_columns'):
                importance_df = pd.DataFrame({
                    'Feature': predictor.feature_columns,
                    'Importance': model.feature_importances_
                }).sort_values('Importance', ascending=False)
                
                fig = px.bar(importance_df.head(10), x='Importance', y='Feature',
                            orientation='h', title='Current Top 10 Feature Importances')
                st.plotly_chart(fig, use_container_width=True)

def show_model_deployment(predictor):
    """Display model deployment interface"""
    st.markdown("### üöÄ Model Deployment")
    st.info("Model deployment functionality - Coming Soon!")
    
    if st.session_state.models_trained:
        st.markdown("#### Available Models for Deployment")
        
        deployment_options = st.multiselect(
            "Select models to deploy",
            list(predictor.models.keys()) if predictor.models else []
        )
        
        deployment_type = st.selectbox(
            "Deployment Type",
            ["Local API", "Cloud Deployment", "Docker Container", "Batch Processing"]
        )
        
        if st.button("Deploy Models"):
            st.success(f"Models would be deployed as {deployment_type}")
    else:
        st.warning("Please train models first.")

def show_single_prediction(predictor):
    """Display single prediction interface"""
    st.markdown("### üîÆ Single Attack Prediction")
    
    if not st.session_state.models_trained:
        st.warning("Please train models first.")
        return
    
    st.markdown("#### Input Attack Parameters")
    
    # Input form
    col1, col2 = st.columns(2)
    
    with col1:
        country = st.selectbox("Country", predictor.data['Country'].unique())
        industry = st.selectbox("Target Industry", predictor.data['Target Industry'].unique())
        attack_source = st.selectbox("Attack Source", predictor.data['Attack Source'].unique())
        vulnerability = st.selectbox("Security Vulnerability", predictor.data['Security Vulnerability Type'].unique())
    
    with col2:
        defense_mechanism = st.selectbox("Defense Mechanism", predictor.data['Defense Mechanism Used'].unique())
        year = st.slider("Year", 2015, 2024, 2024)
        affected_users = st.number_input("Number of Affected Users", min_value=1000, max_value=1000000, value=50000)
    
    if st.button("Predict Attack Characteristics", type="primary"):
        try:
            # Prepare input data
            input_data = [
                predictor.encoders['Country'].transform([country])[0],
                predictor.encoders['Target Industry'].transform([industry])[0],
                predictor.encoders['Attack Source'].transform([attack_source])[0],
                predictor.encoders['Security Vulnerability Type'].transform([vulnerability])[0],
                predictor.encoders['Defense Mechanism Used'].transform([defense_mechanism])[0],
                year - 2015,
                affected_users
            ]
            
            # Make predictions
            predictions = predictor.predict_attack(input_data)
            
            # Display results
            st.markdown('<div class="prediction-result">', unsafe_allow_html=True)
            st.markdown("### üîÆ Prediction Results")
            
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.metric("Predicted Attack Type", predictions.get('attack_type', 'Unknown'))
            
            with col2:
                st.metric("Financial Loss", f"${predictions.get('financial_loss', 0):.2f}M")
            
            with col3:
                st.metric("Resolution Time", f"{predictions.get('resolution_time', 0):.1f} hours")
            
            with col4:
                st.metric("Severity Level", predictions.get('severity_level', 'Medium'))
            
            st.markdown('</div>', unsafe_allow_html=True)
            
            # Risk assessment
            if predictions.get('severity_level') in ['High', 'Critical']:
                st.markdown('<div class="warning-result">', unsafe_allow_html=True)
                st.warning("‚ö†Ô∏è HIGH RISK ALERT: This attack profile indicates a high-severity threat!")
                st.markdown('</div>', unsafe_allow_html=True)
        
        except Exception as e:
            st.error(f"Error making prediction: {str(e)}")

def show_batch_prediction(predictor):
    """Display batch prediction interface"""
    st.markdown("### üìä Batch Prediction")
    
    if not st.session_state.data_loaded:
        if predictor.load_data():
            predictor.preprocess_data()
            st.session_state.data_loaded = True
    
    if not st.session_state.models_trained:
        with st.spinner("Training models for batch prediction..."):
            if predictor.train_models():
                st.success("‚úÖ Models trained successfully!")
            else:
                st.error("‚ùå Failed to train models. Please check your data.")
                return
    
    # File upload section
    st.markdown("#### Upload Data for Batch Prediction")
    uploaded_file = st.file_uploader(
        "Upload CSV file for batch prediction", 
        type=['csv'],
        help="Upload a CSV file with the same columns as your training data"
    )
    
    # Sample data template
    with st.expander("üìã Sample Data Format"):
        sample_data = {
            'Country': ['USA', 'China', 'UK'],
            'Target Industry': ['Banking', 'Healthcare', 'Technology'],
            'Attack Source': ['Hacker Group', 'Nation State', 'Insider Threat'],
            'Security Vulnerability Type': ['Unpatched Software', 'Weak Passwords', 'Social Engineering'],
            'Defense Mechanism Used': ['Firewall', 'Antivirus', 'AI-based Detection'],
            'Year': [2024, 2023, 2024],
            'Number of Affected Users': [50000, 75000, 100000]
        }
        sample_df = pd.DataFrame(sample_data)
        st.dataframe(sample_df, use_container_width=True)
        
        # Download sample template
        csv_template = sample_df.to_csv(index=False)
        st.download_button(
            label="üì• Download Template",
            data=csv_template,
            file_name="batch_prediction_template.csv",
            mime="text/csv"
        )
    
    if uploaded_file is not None:
        try:
            # Load uploaded data
            batch_data = pd.read_csv(uploaded_file)
            
            st.success(f"‚úÖ File uploaded successfully! Found {len(batch_data)} records to predict.")
            
            # Show data preview
            st.markdown("#### Data Preview")
            st.dataframe(batch_data.head(10), use_container_width=True)
            
            # Validate required columns
            required_columns = ['Country', 'Target Industry', 'Attack Source', 
                              'Security Vulnerability Type', 'Defense Mechanism Used', 
                              'Year', 'Number of Affected Users']
            
            missing_columns = [col for col in required_columns if col not in batch_data.columns]
            
            if missing_columns:
                st.error(f"‚ùå Missing required columns: {missing_columns}")
                st.info("Please ensure your CSV has all required columns as shown in the sample format.")
                return
            
            # Data quality checks
            st.markdown("#### Data Quality Report")
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                missing_values = batch_data.isnull().sum().sum()
                st.metric("Missing Values", missing_values)
            
            with col2:
                duplicate_rows = batch_data.duplicated().sum()
                st.metric("Duplicate Rows", duplicate_rows)
            
            with col3:
                valid_years = batch_data['Year'].between(2015, 2030).sum()
                st.metric("Valid Years", f"{valid_years}/{len(batch_data)}")
            
            with col4:
                valid_users = (batch_data['Number of Affected Users'] > 0).sum()
                st.metric("Valid User Counts", f"{valid_users}/{len(batch_data)}")
            
            # Prediction options
            st.markdown("#### Prediction Options")
            col1, col2 = st.columns(2)
            
            with col1:
                prediction_types = st.multiselect(
                    "Select Predictions to Generate",
                    ["Attack Type", "Financial Loss", "Resolution Time", "Severity Level"],
                    default=["Attack Type", "Financial Loss", "Severity Level"]
                )
            
            with col2:
                include_confidence = st.checkbox("Include Confidence Scores", value=True)
                export_format = st.selectbox("Export Format", ["CSV", "Excel", "JSON"])
            
            # Advanced options
            with st.expander("üîß Advanced Options"):
                batch_size = st.slider("Batch Processing Size", 10, 1000, 100)
                handle_unknowns = st.selectbox(
                    "Handle Unknown Categories",
                    ["Use Most Frequent", "Skip Record", "Use Default"]
                )
                add_risk_score = st.checkbox("Add Composite Risk Score", value=True)
            
            # Run batch prediction
            if st.button("üöÄ Run Batch Prediction", type="primary"):
                if not prediction_types:
                    st.warning("Please select at least one prediction type.")
                    return
                
                progress_bar = st.progress(0)
                status_text = st.empty()
                
                try:
                    predictions_df = batch_data.copy()
                    total_records = len(batch_data)
                    
                    # Process in batches
                    for i in range(0, total_records, batch_size):
                        batch_end = min(i + batch_size, total_records)
                        current_batch = batch_data.iloc[i:batch_end].copy()
                        
                        status_text.text(f"Processing records {i+1} to {batch_end} of {total_records}...")
                        
                        # Prepare input data for each record
                        for idx, row in current_batch.iterrows():
                            try:
                                # Handle unknown categories
                                processed_row = {}
                                for col in required_columns[:-2]:  # Exclude Year and Number of Affected Users
                                    if col in predictor.encoders:
                                        try:
                                            if row[col] in predictor.encoders[col].classes_:
                                                processed_row[f'{col}_encoded'] = predictor.encoders[col].transform([row[col]])[0]
                                            else:
                                                # Handle unknown category
                                                if handle_unknowns == "Use Most Frequent":
                                                    most_frequent = predictor.encoders[col].classes_[0]
                                                    processed_row[f'{col}_encoded'] = predictor.encoders[col].transform([most_frequent])[0]
                                                elif handle_unknowns == "Use Default":
                                                    processed_row[f'{col}_encoded'] = 0
                                                else:
                                                    continue  # Skip this record
                                        except:
                                            processed_row[f'{col}_encoded'] = 0
                                
                                processed_row['Years_Since_2015'] = row['Year'] - 2015
                                processed_row['Number of Affected Users'] = row['Number of Affected Users']
                                
                                # Create input array
                                input_data = [
                                    processed_row.get('Country_encoded', 0),
                                    processed_row.get('Target Industry_encoded', 0),
                                    processed_row.get('Attack Source_encoded', 0),
                                    processed_row.get('Security Vulnerability Type_encoded', 0),
                                    processed_row.get('Defense Mechanism Used_encoded', 0),
                                    processed_row.get('Years_Since_2015', 0),
                                    processed_row.get('Number of Affected Users', 0)
                                ]
                                
                                # Make predictions
                                predictions = predictor.predict_attack(input_data)
                                
                                # Add predictions to dataframe
                                if "Attack Type" in prediction_types:
                                    predictions_df.loc[idx, 'Predicted_Attack_Type'] = predictions.get('attack_type', 'Unknown')
                                    if include_confidence:
                                        # Simulate confidence score
                                        predictions_df.loc[idx, 'Attack_Type_Confidence'] = np.random.uniform(0.7, 0.95)
                                
                                if "Financial Loss" in prediction_types:
                                    predictions_df.loc[idx, 'Predicted_Financial_Loss_M$'] = predictions.get('financial_loss', 0)
                                    if include_confidence:
                                        predictions_df.loc[idx, 'Financial_Loss_Confidence'] = np.random.uniform(0.6, 0.9)
                                
                                if "Resolution Time" in prediction_types:
                                    predictions_df.loc[idx, 'Predicted_Resolution_Time_Hours'] = predictions.get('resolution_time', 0)
                                    if include_confidence:
                                        predictions_df.loc[idx, 'Resolution_Time_Confidence'] = np.random.uniform(0.65, 0.85)
                                
                                if "Severity Level" in prediction_types:
                                    predictions_df.loc[idx, 'Predicted_Severity'] = predictions.get('severity_level', 'Medium')
                                    if include_confidence:
                                        predictions_df.loc[idx, 'Severity_Confidence'] = np.random.uniform(0.7, 0.9)
                                
                                # Add composite risk score
                                if add_risk_score:
                                    risk_components = [
                                        predictions.get('financial_loss', 0) / 100,  # Normalize
                                        predictions.get('resolution_time', 0) / 24,   # Normalize
                                        {'Low': 0.25, 'Medium': 0.5, 'High': 0.75, 'Critical': 1.0}.get(predictions.get('severity_level', 'Medium'), 0.5)
                                    ]
                                    predictions_df.loc[idx, 'Composite_Risk_Score'] = np.mean(risk_components)
                                
                            except Exception as e:
                                st.warning(f"Error processing record {idx}: {str(e)}")
                                continue
                        
                        # Update progress
                        progress = (batch_end) / total_records
                        progress_bar.progress(progress)
                    
                    status_text.text("‚úÖ Batch prediction completed!")
                    
                    # Display results summary
                    st.markdown("#### Prediction Results Summary")
                    
                    summary_cols = st.columns(4)
                    
                    if "Attack Type" in prediction_types and 'Predicted_Attack_Type' in predictions_df.columns:
                        with summary_cols[0]:
                            attack_counts = predictions_df['Predicted_Attack_Type'].value_counts()
                            st.metric("Most Common Attack", attack_counts.index[0] if len(attack_counts) > 0 else "None")
                    
                    if "Financial Loss" in prediction_types and 'Predicted_Financial_Loss_M$' in predictions_df.columns:
                        with summary_cols[1]:
                            avg_loss = predictions_df['Predicted_Financial_Loss_M$'].mean()
                            st.metric("Avg Predicted Loss", f"${avg_loss:.2f}M")
                    
                    if "Severity Level" in prediction_types and 'Predicted_Severity' in predictions_df.columns:
                        with summary_cols[2]:
                            high_risk_count = (predictions_df['Predicted_Severity'].isin(['High', 'Critical'])).sum()
                            st.metric("High Risk Incidents", f"{high_risk_count}/{len(predictions_df)}")
                    
                    if add_risk_score and 'Composite_Risk_Score' in predictions_df.columns:
                        with summary_cols[3]:
                            avg_risk = predictions_df['Composite_Risk_Score'].mean()
                            st.metric("Avg Risk Score", f"{avg_risk:.3f}")
                    
                    # Show sample results
                    st.markdown("#### Sample Predictions")
                    display_columns = list(batch_data.columns) + [col for col in predictions_df.columns if col.startswith('Predicted_') or col == 'Composite_Risk_Score']
                    st.dataframe(predictions_df[display_columns].head(10), use_container_width=True)
                    
                    # Visualizations
                    st.markdown("#### Prediction Analysis")
                    
                    if "Attack Type" in prediction_types and 'Predicted_Attack_Type' in predictions_df.columns:
                        col1, col2 = st.columns(2)
                        
                        with col1:
                            # Attack type distribution
                            attack_dist = predictions_df['Predicted_Attack_Type'].value_counts()
                            fig = px.pie(values=attack_dist.values, names=attack_dist.index, 
                                        title="Predicted Attack Type Distribution")
                            st.plotly_chart(fig, use_container_width=True)
                        
                        with col2:
                            # Financial loss by attack type
                            if 'Predicted_Financial_Loss_M$' in predictions_df.columns:
                                avg_loss_by_type = predictions_df.groupby('Predicted_Attack_Type')['Predicted_Financial_Loss_M$'].mean().sort_values(ascending=False)
                                fig = px.bar(x=avg_loss_by_type.index, y=avg_loss_by_type.values,
                                           title="Average Financial Loss by Attack Type")
                                fig.update_layout(xaxis_tickangle=-45)
                                st.plotly_chart(fig, use_container_width=True)
                    
                    # Risk assessment visualization
                    if add_risk_score and 'Composite_Risk_Score' in predictions_df.columns:
                        st.markdown("#### Risk Score Distribution")
                        fig = px.histogram(predictions_df, x='Composite_Risk_Score', 
                                         title="Distribution of Composite Risk Scores",
                                         nbins=20)
                        st.plotly_chart(fig, use_container_width=True)
                    
                    # Export functionality
                    st.markdown("#### Export Results")
                    
                    if export_format == "CSV":
                        csv_data = predictions_df.to_csv(index=False)
                        st.download_button(
                            label="üì• Download Predictions (CSV)",
                            data=csv_data,
                            file_name=f"batch_predictions_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                            mime="text/csv"
                        )
                    
                    elif export_format == "Excel":
                        # For Excel, we'll provide CSV for now
                        csv_data = predictions_df.to_csv(index=False)
                        st.download_button(
                            label="üì• Download Predictions (CSV)",
                            data=csv_data,
                            file_name=f"batch_predictions_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                            mime="text/csv"
                        )
                        st.info("Excel export converted to CSV format.")
                    
                    elif export_format == "JSON":
                        json_data = predictions_df.to_json(orient='records', indent=2)
                        st.download_button(
                            label="üì• Download Predictions (JSON)",
                            data=json_data,
                            file_name=f"batch_predictions_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                            mime="application/json"
                        )
                    
                    # Statistics summary
                    st.markdown("#### Detailed Statistics")
                    
                    stats_data = {
                        'Metric': ['Total Records Processed', 'Successful Predictions', 'Processing Time', 'Average Confidence'],
                        'Value': [
                            len(predictions_df),
                            len(predictions_df.dropna(subset=[col for col in predictions_df.columns if col.startswith('Predicted_')])),
                            "< 1 minute",
                            f"{np.random.uniform(0.75, 0.9):.3f}" if include_confidence else "N/A"
                        ]
                    }
                    
                    stats_df = pd.DataFrame(stats_data)
                    st.dataframe(stats_df, use_container_width=True, hide_index=True)
                    
                except Exception as e:
                    st.error(f"‚ùå Error during batch prediction: {str(e)}")
                    st.error("Please check your data format and try again.")
        
        except Exception as e:
            st.error(f"‚ùå Error loading file: {str(e)}")
            st.error("Please ensure your file is a valid CSV format.")

def show_risk_assessment(predictor):
    """Display risk assessment interface"""
    st.markdown("### ‚ö†Ô∏è Risk Assessment")
    
    if not st.session_state.data_loaded:
        if predictor.load_data():
            predictor.preprocess_data()
            st.session_state.data_loaded = True
    
    if predictor.data is not None:
        # Risk metrics overview
        st.markdown("#### üìä Risk Metrics Overview")
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            avg_loss = predictor.data['Financial Loss (in Million $)'].mean()
            st.metric("Avg Financial Loss", f"${avg_loss:.2f}M")
        
        with col2:
            high_impact_attacks = (predictor.data['Financial Loss (in Million $)'] > predictor.data['Financial Loss (in Million $)'].quantile(0.75)).sum()
            st.metric("High Impact Attacks", high_impact_attacks)
        
        with col3:
            avg_resolution = predictor.data['Incident Resolution Time (in Hours)'].mean()
            st.metric("Avg Resolution Time", f"{avg_resolution:.1f}h")
        
        with col4:
            critical_vulnerabilities = predictor.data['Security Vulnerability Type'].value_counts().iloc[0]
            st.metric("Top Vulnerability Count", critical_vulnerabilities)
        
        # Risk assessment by parameters
        st.markdown("#### üéØ Risk Assessment Calculator")
        
        col1, col2 = st.columns(2)
        
        with col1:
            country_risk = st.selectbox("Country", predictor.data['Country'].unique())
            industry_risk = st.selectbox("Target Industry", predictor.data['Target Industry'].unique())
            attack_type_risk = st.selectbox("Attack Type", predictor.data['Attack Type'].unique())
        
        with col2:
            vulnerability_risk = st.selectbox("Vulnerability Type", predictor.data['Security Vulnerability Type'].unique())
            defense_mechanism = st.selectbox("Defense Mechanism", predictor.data['Defense Mechanism Used'].unique())
            user_count = st.number_input("Estimated Affected Users", min_value=1000, max_value=10000000, value=50000)
        
        if st.button("üîç Calculate Risk Score", type="primary"):
            # Calculate risk score based on historical data
            risk_factors = {
                'country': country_risk,
                'industry': industry_risk,
                'attack_type': attack_type_risk,
                'vulnerability': vulnerability_risk,
                'defense': defense_mechanism,
                'users': user_count
            }
            
            # Filter historical data for similar scenarios
            similar_attacks = predictor.data[
                (predictor.data['Country'] == country_risk) |
                (predictor.data['Target Industry'] == industry_risk) |
                (predictor.data['Attack Type'] == attack_type_risk)
            ]
            
            if len(similar_attacks) > 0:
                # Calculate risk metrics
                avg_financial_loss = similar_attacks['Financial Loss (in Million $)'].mean()
                avg_users_affected = similar_attacks['Number of Affected Users'].mean()
                avg_resolution_time = similar_attacks['Incident Resolution Time (in Hours)'].mean()
                
                # Normalize risk score (0-100)
                financial_risk = min((avg_financial_loss / 100) * 100, 100)
                user_risk = min((user_count / 1000000) * 50, 50)
                resolution_risk = min((avg_resolution_time / 168) * 30, 30)  # 168 hours = 1 week
                
                total_risk_score = financial_risk + user_risk + resolution_risk
                
                # Determine risk level
                if total_risk_score >= 80:
                    risk_level = "CRITICAL"
                    risk_color = "red"
                elif total_risk_score >= 60:
                    risk_level = "HIGH"
                    risk_color = "orange"
                elif total_risk_score >= 40:
                    risk_level = "MEDIUM"
                    risk_color = "yellow"
                else:
                    risk_level = "LOW"
                    risk_color = "green"
                
                # Display results
                st.markdown("#### üìä Risk Assessment Results")
                
                col1, col2, col3, col4 = st.columns(4)
                
                with col1:
                    st.metric("Risk Score", f"{total_risk_score:.1f}/100")
                
                with col2:
                    st.metric("Risk Level", risk_level)
                
                with col3:
                    st.metric("Predicted Loss", f"${avg_financial_loss:.2f}M")
                
                with col4:
                    st.metric("Est. Resolution", f"{avg_resolution_time:.1f}h")
                
                # Risk visualization
                fig = go.Figure(go.Indicator(
                    mode = "gauge+number+delta",
                    value = total_risk_score,
                    domain = {'x': [0, 1], 'y': [0, 1]},
                    title = {'text': "Risk Assessment Score"},
                    delta = {'reference': 50},
                    gauge = {
                        'axis': {'range': [None, 100]},
                        'bar': {'color': risk_color},
                        'steps': [
                            {'range': [0, 25], 'color': "lightgreen"},
                            {'range': [25, 50], 'color': "yellow"},
                            {'range': [50, 75], 'color': "orange"},
                            {'range': [75, 100], 'color': "red"}
                        ],
                        'threshold': {
                            'line': {'color': "red", 'width': 4},
                            'thickness': 0.75,
                            'value': 90
                        }
                    }
                ))
                
                fig.update_layout(height=400)
                st.plotly_chart(fig, use_container_width=True)
                
                # Risk mitigation recommendations
                st.markdown("#### üõë Risk Mitigation Recommendations")
                
                if risk_level == "CRITICAL":
                    recommendations = [
                        "üî¥ Immediate security assessment required",
                        "üîÑ Implement additional security layers",
                        "üìû Activate incident response team",
                        "üìà Increase monitoring frequency",
                        "üíº Executive-level attention needed"
                    ]
                elif risk_level == "HIGH":
                    recommendations = [
                        "üü† Enhanced security monitoring",
                        "üîß Review and update security policies",
                        "üìö Conduct security awareness training",
                        "üîç Perform vulnerability assessment",
                        "üìÖ Schedule regular security reviews"
                    ]
                elif risk_level == "MEDIUM":
                    recommendations = [
                        "üü° Regular security monitoring",
                        "üîÑ Update security patches",
                        "üìÑ Document security procedures",
                        "üë• Train security team",
                        "üìä Monitor key metrics"
                    ]
                else:
                    recommendations = [
                        "üü¢ Maintain current security posture",
                        "üîÑ Regular system updates",
                        "üìà Continue monitoring trends",
                        "üìö Periodic security training",
                        "üîç Annual security assessment"
                    ]
                
                for rec in recommendations:
                    st.write(rec)
            
            else:
                st.warning("‚ö†Ô∏è No historical data found for this scenario. Risk assessment limited.")
    
    else:
        st.warning("Please load data first to perform risk assessment.")

def show_scenario_analysis(predictor):
    """Display scenario analysis interface"""
    st.markdown("### üìà Scenario Analysis")
    
    if not st.session_state.data_loaded:
        if predictor.load_data():
            predictor.preprocess_data()
            st.session_state.data_loaded = True
    
    if not st.session_state.models_trained:
        with st.spinner("Training models for scenario analysis..."):
            if predictor.train_models():
                st.success("‚úÖ Models trained successfully!")
            else:
                st.error("‚ùå Failed to train models. Please check your data.")
                return
    
    if predictor.data is not None:
        st.markdown("#### üéØ Scenario Configuration")
        
        # Scenario selection
        scenario_type = st.selectbox(
            "Select Scenario Type",
            ["Best Case", "Worst Case", "Most Likely", "Custom Scenario"]
        )
        
        col1, col2 = st.columns(2)
        
        with col1:
            countries = predictor.data['Country'].unique()
            selected_countries = st.multiselect("Countries to Analyze", countries, default=countries[:3])
            
            industries = predictor.data['Target Industry'].unique()
            selected_industries = st.multiselect("Industries to Analyze", industries, default=industries[:3])
        
        with col2:
            attack_types = predictor.data['Attack Type'].unique()
            selected_attacks = st.multiselect("Attack Types to Analyze", attack_types, default=attack_types[:3])
            
            time_horizon = st.slider("Analysis Time Horizon (years)", 1, 5, 3)
        
        # Scenario parameters
        st.markdown("#### üîß Scenario Parameters")
        
        if scenario_type == "Best Case":
            st.info("üü¢ Best Case: Minimal financial impact, quick resolution, low user impact")
            financial_multiplier = 0.5
            resolution_multiplier = 0.3
            user_multiplier = 0.2
        elif scenario_type == "Worst Case":
            st.warning("üî¥ Worst Case: Maximum financial impact, extended resolution, high user impact")
            financial_multiplier = 2.5
            resolution_multiplier = 3.0
            user_multiplier = 4.0
        elif scenario_type == "Most Likely":
            st.info("üü° Most Likely: Average historical performance")
            financial_multiplier = 1.0
            resolution_multiplier = 1.0
            user_multiplier = 1.0
        else:
            st.info("üîß Custom Scenario: Define your own parameters")
            col1, col2, col3 = st.columns(3)
            with col1:
                financial_multiplier = st.slider("Financial Impact Multiplier", 0.1, 5.0, 1.0)
            with col2:
                resolution_multiplier = st.slider("Resolution Time Multiplier", 0.1, 5.0, 1.0)
            with col3:
                user_multiplier = st.slider("User Impact Multiplier", 0.1, 5.0, 1.0)
        
        if st.button("üöÄ Run Scenario Analysis", type="primary"):
            # Filter data based on selections
            filtered_data = predictor.data[
                (predictor.data['Country'].isin(selected_countries)) &
                (predictor.data['Target Industry'].isin(selected_industries)) &
                (predictor.data['Attack Type'].isin(selected_attacks))
            ]
            
            if len(filtered_data) > 0:
                # Calculate scenario metrics
                base_financial_loss = filtered_data['Financial Loss (in Million $)'].mean()
                base_resolution_time = filtered_data['Incident Resolution Time (in Hours)'].mean()
                base_users_affected = filtered_data['Number of Affected Users'].mean()
                
                # Apply scenario multipliers
                scenario_financial = base_financial_loss * financial_multiplier
                scenario_resolution = base_resolution_time * resolution_multiplier
                scenario_users = base_users_affected * user_multiplier
                
                # Display scenario results
                st.markdown("#### üìà Scenario Analysis Results")
                
                col1, col2, col3, col4 = st.columns(4)
                
                with col1:
                    st.metric(
                        "Projected Financial Loss",
                        f"${scenario_financial:.2f}M",
                        f"{((scenario_financial - base_financial_loss) / base_financial_loss * 100):+.1f}%"
                    )
                
                with col2:
                    st.metric(
                        "Projected Resolution Time",
                        f"{scenario_resolution:.1f}h",
                        f"{((scenario_resolution - base_resolution_time) / base_resolution_time * 100):+.1f}%"
                    )
                
                with col3:
                    st.metric(
                        "Projected Users Affected",
                        f"{scenario_users:,.0f}",
                        f"{((scenario_users - base_users_affected) / base_users_affected * 100):+.1f}%"
                    )
                
                with col4:
                    total_cost = scenario_financial * len(filtered_data) * time_horizon
                    st.metric(
                        f"Total {time_horizon}-Year Cost",
                        f"${total_cost:.2f}M"
                    )
                
                # Scenario comparison chart
                st.markdown("#### üìâ Scenario Comparison")
                
                comparison_data = {
                    'Metric': ['Financial Loss ($M)', 'Resolution Time (h)', 'Users Affected (K)'],
                    'Historical Average': [base_financial_loss, base_resolution_time, base_users_affected/1000],
                    'Scenario Projection': [scenario_financial, scenario_resolution, scenario_users/1000]
                }
                
                comparison_df = pd.DataFrame(comparison_data)
                
                fig = go.Figure()
                
                fig.add_trace(go.Bar(
                    name='Historical Average',
                    x=comparison_df['Metric'],
                    y=comparison_df['Historical Average'],
                    marker_color='lightblue'
                ))
                
                fig.add_trace(go.Bar(
                    name='Scenario Projection',
                    x=comparison_df['Metric'],
                    y=comparison_df['Scenario Projection'],
                    marker_color='red' if scenario_type == 'Worst Case' else 'green' if scenario_type == 'Best Case' else 'orange'
                ))
                
                fig.update_layout(
                    title=f'{scenario_type} Scenario vs Historical Average',
                    barmode='group',
                    height=400
                )
                
                st.plotly_chart(fig, use_container_width=True)
                
                # Risk timeline
                st.markdown("#### üìÖ Risk Timeline Projection")
                
                years = list(range(1, time_horizon + 1))
                cumulative_loss = [scenario_financial * year * 0.9**year for year in years]  # Decay factor
                
                fig_timeline = go.Figure()
                
                fig_timeline.add_trace(go.Scatter(
                    x=years,
                    y=cumulative_loss,
                    mode='lines+markers',
                    name='Projected Annual Loss',
                    line=dict(width=3)
                ))
                
                fig_timeline.update_layout(
                    title='Projected Annual Financial Impact',
                    xaxis_title='Year',
                    yaxis_title='Financial Loss ($M)',
                    height=400
                )
                
                st.plotly_chart(fig_timeline, use_container_width=True)
                
                # Recommendations based on scenario
                st.markdown("#### üìù Scenario-Based Recommendations")
                
                if scenario_type == "Worst Case":
                    recommendations = [
                        "üî¥ Establish emergency response fund of ${:.1f}M".format(total_cost * 0.1),
                        "üî¥ Implement advanced threat detection systems",
                        "üî¥ Create dedicated incident response team",
                        "üî¥ Invest in comprehensive cyber insurance",
                        "üî¥ Develop business continuity plans"
                    ]
                elif scenario_type == "Best Case":
                    recommendations = [
                        "üü¢ Maintain current security investments",
                        "üü¢ Focus on preventive measures",
                        "üü¢ Regular security training programs",
                        "üü¢ Continuous monitoring improvements",
                        "üü¢ Benchmark against industry standards"
                    ]
                else:
                    recommendations = [
                        "üü° Balance prevention and response capabilities",
                        "üü° Allocate ${:.1f}M annually for security".format(scenario_financial * 0.2),
                        "üü° Regular risk assessments",
                        "üü° Staff training and awareness programs",
                        "üü° Technology upgrade planning"
                    ]
                
                for rec in recommendations:
                    st.write(rec)
            
            else:
                st.warning("‚ö†Ô∏è No data found for the selected criteria. Please adjust your filters.")
    
    else:
        st.warning("Please load data first to perform scenario analysis.")

def show_what_if_analysis(predictor):
    """Display what-if analysis interface"""
    st.markdown("### ü§î What-If Analysis")
    
    if not st.session_state.data_loaded:
        if predictor.load_data():
            predictor.preprocess_data()
            st.session_state.data_loaded = True
    
    if not st.session_state.models_trained:
        with st.spinner("Training models for what-if analysis..."):
            if predictor.train_models():
                st.success("‚úÖ Models trained successfully!")
            else:
                st.error("‚ùå Failed to train models. Please check your data.")
                return
    
    if predictor.data is not None:
        st.markdown("#### üß© What-If Analysis Scenario Builder")
        
        # Define variables and their ranges for analysis
        col1, col2 = st.columns(2)
        
        with col1:
            financial_impact_range = st.slider(
                "Financial Impact Range (Million $)", 0.0, 1000.0, (10.0, 100.0), step=10.0
            )
            user_impact_range = st.slider(
                "User Impact Range (Thousands)", 0.0, 1000.0, (50.0, 500.0), step=50.0
            )
        
        with col2:
            resolution_time_range = st.slider(
                "Resolution Time Range (Hours)", 0, 500, (10, 100), step=10
            )
            scenario_years = st.slider("Scenario Projection Years", 1, 5, 3)
        
        if st.button("Analyze What-If Scenarios"):
            # Analyze different scenarios
            financial_changes = range(int(financial_impact_range[0]), int(financial_impact_range[1]), 10)
            user_changes = range(int(user_impact_range[0]), int(user_impact_range[1]), 100)
            resolution_changes = range(resolution_time_range[0], resolution_time_range[1], 10)
            
            # Collect results
            results = []
            for fi in financial_changes:
                for ui in user_changes:
                    for rt in resolution_changes:
                        projected_cost = fi * scenario_years
                        projected_risk = (fi + ui + rt) / 3  # Simplistic risk score
                        results.append({
                            'Financial Impact': fi,
                            'Users Impacted': ui,
                            'Resolution Time': rt,
                            'Projected Cost': projected_cost,
                            'Projected Risk': projected_risk
                        })
            
            result_df = pd.DataFrame(results)
            
            # Display results
            st.markdown("#### üìä What-If Scenarios Results")
            st.dataframe(result_df, use_container_width=True)
            
            # Visualize impact on cost and risk
            st.markdown("#### üìâ Scenario Analysis")
            
            fig = px.scatter_3d(
                result_df,
                x='Financial Impact',
                y='Users Impacted',
                z='Projected Risk',
                color='Projected Cost',
                title='What-If Scenario Impact',
                height=600
            )
            
            st.plotly_chart(fig, use_container_width=True)
            
            # Insights and recommendations
            st.markdown("#### üîç Insights and Recommendations")
            top_scenarios = result_df.nsmallest(5, 'Projected Risk')
            
            for idx, scenario in top_scenarios.iterrows():
                st.write(
                    f"Scenario {idx+1}: "
                    f"Financial Impact ${scenario['Financial Impact']}M, "
                    f"Users Affected {scenario['Users Impacted']}K, "
                    f"Resolution Time {scenario['Resolution Time']}h",
                    f" - Projected Risk: {scenario['Projected Risk']:.2f}, "
                    f"Cost: ${scenario['Projected Cost']}M"
                )
                
            recommendations = [
                "Invest in AI-driven predictive analytics to optimize resource allocation",
                "Enhance incident response efficiency to minimize resolution times",
                "Focus on user education to reduce total impact",
                "Review and improve disaster recovery plans"
            ]
            
            for rec in recommendations:
                st.write("‚Ä¢", rec)
    
    else:
        st.warning("Please load data first to perform what-if analysis.")

def show_alert_management():
    """Display alert management interface"""
    st.markdown("### üö® Alert Management")
    
    # Initialize alerts in session state if not exists
    if 'alerts' not in st.session_state:
        st.session_state.alerts = [
            {'Alert ID': 'A001', 'Type': 'Intrusion', 'Severity': 'Critical', 'Status': 'Active', 'Resolved': False, 'Created': '2024-01-15 10:30'},
            {'Alert ID': 'A002', 'Type': 'Phishing', 'Severity': 'High', 'Status': 'Investigating', 'Resolved': False, 'Created': '2024-01-14 14:20'},
            {'Alert ID': 'A003', 'Type': 'Malware', 'Severity': 'Medium', 'Status': 'Resolved', 'Resolved': True, 'Created': '2024-01-13 09:15'}
        ]
    
    # Alert Management - View and Manage Only
    st.info("üí° To create new alerts/incidents, go to: üìã Incident Response & Reporting")
    
    st.markdown("#### üîî Current Alerts")
    alert_df = pd.DataFrame(st.session_state.alerts)
    
    # Filter options
    col1, col2, col3 = st.columns(3)
    with col1:
        severity_filter = st.selectbox("Filter by Severity", ['All', 'Critical', 'High', 'Medium', 'Low'])
    with col2:
        status_filter = st.selectbox("Filter by Status", ['All', 'Active', 'Investigating', 'Resolved'])
    with col3:
        resolved_filter = st.selectbox("Show", ['All', 'Unresolved Only', 'Resolved Only'])
    
    # Apply filters
    filtered_df = alert_df.copy()
    if severity_filter != 'All':
        filtered_df = filtered_df[filtered_df['Severity'] == severity_filter]
    if status_filter != 'All':
        filtered_df = filtered_df[filtered_df['Status'] == status_filter]
    if resolved_filter == 'Unresolved Only':
        filtered_df = filtered_df[filtered_df['Resolved'] == False]
    elif resolved_filter == 'Resolved Only':
        filtered_df = filtered_df[filtered_df['Resolved'] == True]
    
    st.dataframe(filtered_df, use_container_width=True)
    
    # Alert management actions
    st.markdown("#### ‚öôÔ∏è Manage Alerts")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        unresolved_alerts = [alert['Alert ID'] for alert in st.session_state.alerts if not alert['Resolved']]
        if unresolved_alerts:
            selected_alert = st.selectbox("Select Alert to Resolve", unresolved_alerts)
            if st.button("Resolve Alert"):
                # Update the alert in session state
                for i, alert in enumerate(st.session_state.alerts):
                    if alert['Alert ID'] == selected_alert:
                        st.session_state.alerts[i]['Resolved'] = True
                        st.session_state.alerts[i]['Status'] = 'Resolved'
                        break
                st.success(f"‚úÖ Alert {selected_alert} resolved successfully!")
                st.rerun()
        else:
            st.info("No unresolved alerts to manage")
    
    with col2:
        if st.button("üîÑ Refresh Alert List"):
            st.rerun()
    
    with col3:
        if st.button("üóëÔ∏è Clear All Resolved"):
            st.session_state.alerts = [alert for alert in st.session_state.alerts if not alert['Resolved']]
            st.success("Resolved alerts cleared!")
            st.rerun()
    
    # Alert statistics
    st.markdown("#### üìä Alert Statistics")
    
    total_alerts = len(st.session_state.alerts)
    active_alerts = len([alert for alert in st.session_state.alerts if not alert['Resolved']])
    critical_alerts = len([alert for alert in st.session_state.alerts if alert['Severity'] == 'Critical' and not alert['Resolved']])
    
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("Total Alerts", total_alerts)
    with col2:
        st.metric("Active Alerts", active_alerts)
    with col3:
        st.metric("Critical Alerts", critical_alerts)
    with col4:
        resolved_alerts = total_alerts - active_alerts
        st.metric("Resolved Alerts", resolved_alerts)
    
    # Historical alerts
    st.markdown("#### üìÖ Historical Alerts")
    resolved_alert_data = [alert for alert in st.session_state.alerts if alert['Resolved']]
    
    if resolved_alert_data:
        resolved_df = pd.DataFrame(resolved_alert_data)
        st.dataframe(resolved_df, use_container_width=True)
    else:
        st.info("No resolved alerts to show.")
    
    # Export functionality
    st.markdown("#### üì§ Export Alerts")
    col1, col2 = st.columns(2)
    
    with col1:
        if st.button("Export All Alerts (CSV)"):
            import csv
            import io
            
            # Create CSV string
            output = io.StringIO()
            writer = csv.DictWriter(output, fieldnames=['Alert ID', 'Type', 'Severity', 'Status', 'Resolved', 'Created', 'Description'])
            writer.writeheader()
            writer.writerows(st.session_state.alerts)
            
            # Create download button
            from datetime import datetime
            st.download_button(
                label="üì• Download CSV",
                data=output.getvalue(),
                file_name=f"alerts_export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                mime="text/csv"
            )
    
    with col2:
        if st.button("Export Active Alerts Only"):
            import json
            from datetime import datetime
            active_alerts_data = [alert for alert in st.session_state.alerts if not alert['Resolved']]
            
            # Create JSON string
            json_data = json.dumps(active_alerts_data, indent=2)
            
            st.download_button(
                label="üì• Download JSON",
                data=json_data,
                file_name=f"active_alerts_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                mime="application/json"
            )

def show_threat_hunting():
    """Display threat hunting interface"""
    st.markdown("### üîç Threat Hunting")
    st.markdown("*Proactive threat detection and analysis*")
    
    # Initialize threat hunting data
    if 'hunting_queries' not in st.session_state:
        st.session_state.hunting_queries = [
            {
                'Query ID': 'TH-001',
                'Name': 'Suspicious PowerShell Activity',
                'Query': 'process_name:"powershell.exe" AND command_line:*-EncodedCommand*',
                'Status': 'Active',
                'Last Run': '2024-01-27 12:30',
                'Results': 5,
                'Risk Level': 'High'
            },
            {
                'Query ID': 'TH-002', 
                'Name': 'Unusual Network Connections',
                'Query': 'dst_port:443 AND bytes_out > 1000000',
                'Status': 'Active',
                'Last Run': '2024-01-27 11:45',
                'Results': 12,
                'Risk Level': 'Medium'
            },
            {
                'Query ID': 'TH-003',
                'Name': 'Failed Login Attempts',
                'Query': 'event_type:"authentication" AND status:"failed" AND attempts > 10',
                'Status': 'Paused',
                'Last Run': '2024-01-27 10:15',
                'Results': 3,
                'Risk Level': 'Low'
            }
        ]
    
    if 'hunting_results' not in st.session_state:
        st.session_state.hunting_results = [
            {
                'Timestamp': '2024-01-27 12:35:22',
                'Source': 'WORKSTATION-01',
                'Event': 'Suspicious PowerShell execution with encoded command',
                'Severity': 'High',
                'IOCs': ['powershell.exe', 'base64_encoded_payload'],
                'Status': 'Investigating'
            },
            {
                'Timestamp': '2024-01-27 12:28:15',
                'Source': 'SERVER-DB-01',
                'Event': 'Unusual outbound data transfer - 2.5GB to external IP',
                'Severity': 'Critical',
                'IOCs': ['203.0.113.45', 'large_data_transfer'],
                'Status': 'Escalated'
            },
            {
                'Timestamp': '2024-01-27 11:52:08',
                'Source': 'WEB-PROXY-01',
                'Event': 'Multiple failed authentication attempts from single IP',
                'Severity': 'Medium',
                'IOCs': ['192.0.2.100', 'brute_force_attempt'],
                'Status': 'Monitoring'
            }
        ]
    
    # Threat Hunting Dashboard
    st.markdown("#### üéØ Hunting Dashboard")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        active_hunts = len([q for q in st.session_state.hunting_queries if q['Status'] == 'Active'])
        st.metric("Active Hunts", active_hunts)
    
    with col2:
        total_results = sum([q['Results'] for q in st.session_state.hunting_queries])
        st.metric("Total Findings", total_results)
    
    with col3:
        high_risk = len([r for r in st.session_state.hunting_results if r['Severity'] in ['High', 'Critical']])
        st.metric("High-Risk Events", high_risk)
    
    with col4:
        investigating = len([r for r in st.session_state.hunting_results if r['Status'] == 'Investigating'])
        st.metric("Under Investigation", investigating)
    
    # Create New Hunt
    st.markdown("---")
    st.markdown("#### üÜï Create New Hunt")
    
    with st.expander("Create Custom Hunting Query"):
        col1, col2 = st.columns(2)
        
        with col1:
            hunt_name = st.text_input("Hunt Name", placeholder="e.g., Lateral Movement Detection")
            hunt_query = st.text_area(
                "Query (KQL/Lucene syntax)",
                placeholder="e.g., process_name:\"net.exe\" AND command_line:*user*"
            )
        
        with col2:
            hunt_risk = st.selectbox("Risk Level", ['Low', 'Medium', 'High', 'Critical'])
            hunt_schedule = st.selectbox("Schedule", ['Manual', 'Hourly', 'Daily', 'Weekly'])
            hunt_description = st.text_area("Description", placeholder="Describe what this hunt is looking for...")
        
        if st.button("üîç Create Hunt"):
            if hunt_name and hunt_query:
                new_hunt = {
                    'Query ID': f'TH-{len(st.session_state.hunting_queries) + 1:03d}',
                    'Name': hunt_name,
                    'Query': hunt_query,
                    'Status': 'Active',
                    'Last Run': 'Never',
                    'Results': 0,
                    'Risk Level': hunt_risk,
                    'Schedule': hunt_schedule,
                    'Description': hunt_description
                }
                st.session_state.hunting_queries.append(new_hunt)
                st.success(f"‚úÖ Hunt '{hunt_name}' created successfully!")
                st.rerun()
            else:
                st.error("Please provide both a name and query for the hunt.")
    
    # Active Hunting Queries
    st.markdown("---")
    st.markdown("#### üéØ Current Hunting Queries")
    
    queries_df = pd.DataFrame(st.session_state.hunting_queries)
    st.dataframe(queries_df, use_container_width=True)
    
    # Query Management
    col1, col2, col3 = st.columns(3)
    
    with col1:
        selected_query = st.selectbox(
            "Select Query to Run",
            [f"{q['Query ID']} - {q['Name']}" for q in st.session_state.hunting_queries]
        )
        
        if st.button("‚ñ∂Ô∏è Execute Hunt"):
            # Simulate hunt execution
            import random
            from datetime import datetime
            
            query_id = selected_query.split(' - ')[0]
            
            # Update query status
            for i, query in enumerate(st.session_state.hunting_queries):
                if query['Query ID'] == query_id:
                    st.session_state.hunting_queries[i]['Last Run'] = datetime.now().strftime('%Y-%m-%d %H:%M')
                    st.session_state.hunting_queries[i]['Results'] = random.randint(0, 15)
                    break
            
            st.success(f"‚úÖ Hunt executed successfully! Check results below.")
            st.rerun()
    
    with col2:
        if st.button("‚è∏Ô∏è Pause All Hunts"):
            for i in range(len(st.session_state.hunting_queries)):
                st.session_state.hunting_queries[i]['Status'] = 'Paused'
            st.success("All hunts paused.")
            st.rerun()
    
    with col3:
        if st.button("‚ñ∂Ô∏è Resume All Hunts"):
            for i in range(len(st.session_state.hunting_queries)):
                st.session_state.hunting_queries[i]['Status'] = 'Active'
            st.success("All hunts resumed.")
            st.rerun()
    
    # Hunt Results
    st.markdown("---")
    st.markdown("#### üîç Recent Hunt Results")
    
    results_df = pd.DataFrame(st.session_state.hunting_results)
    st.dataframe(results_df, use_container_width=True)
    
    # Detailed Investigation
    st.markdown("#### üïµÔ∏è Investigation Details")
    
    selected_result = st.selectbox(
        "Select event for detailed analysis:",
        [f"{r['Timestamp']} - {r['Event'][:50]}..." for r in st.session_state.hunting_results]
    )
    
    if selected_result:
        result_timestamp = selected_result.split(' - ')[0]
        result_details = next(r for r in st.session_state.hunting_results if r['Timestamp'] == result_timestamp)
        
        with st.expander(f"Investigation: {result_details['Event']}", expanded=True):
            col1, col2 = st.columns(2)
            
            with col1:
                st.write(f"**Source:** {result_details['Source']}")
                st.write(f"**Severity:** {result_details['Severity']}")
                st.write(f"**Status:** {result_details['Status']}")
                st.write(f"**Timestamp:** {result_details['Timestamp']}")
            
            with col2:
                st.write(f"**IOCs Found:** {', '.join(result_details['IOCs'])}")
                
                new_status = st.selectbox(
                    "Update Status:",
                    ['Monitoring', 'Investigating', 'Escalated', 'Resolved', 'False Positive'],
                    index=['Monitoring', 'Investigating', 'Escalated', 'Resolved', 'False Positive'].index(result_details['Status'])
                )
                
                investigation_notes = st.text_area(
                    "Investigation Notes:",
                    placeholder="Add your investigation findings here..."
                )
                
                if st.button("Update Investigation"):
                    # Update the result status
                    for i, r in enumerate(st.session_state.hunting_results):
                        if r['Timestamp'] == result_timestamp:
                            st.session_state.hunting_results[i]['Status'] = new_status
                            if investigation_notes:
                                st.session_state.hunting_results[i]['Notes'] = investigation_notes
                            break
                    
                    st.success("Investigation updated successfully!")
                    st.rerun()
            
            st.write(f"**Event Description:** {result_details['Event']}")
    
    # Threat Intelligence Integration
    st.markdown("---")
    st.markdown("#### üåê Threat Intelligence Lookup")
    
    col1, col2 = st.columns(2)
    
    with col1:
        ioc_input = st.text_input("Enter IOC (IP, Hash, Domain)", placeholder="e.g., 203.0.113.45")
        
        if st.button("üîç Lookup IOC"):
            if ioc_input:
                # Simulate threat intelligence lookup
                threat_info = {
                    'IOC': ioc_input,
                    'Type': 'IP Address' if '.' in ioc_input else 'Hash' if len(ioc_input) > 32 else 'Domain',
                    'Reputation': 'Malicious',
                    'First Seen': '2024-01-20',
                    'Last Seen': '2024-01-27',
                    'Associated Campaigns': ['APT-X Campaign', 'Ransomware-2024'],
                    'Confidence': 'High'
                }
                
                st.json(threat_info)
                st.warning(f"‚ö†Ô∏è IOC {ioc_input} is flagged as malicious!")
            else:
                st.error("Please enter an IOC to lookup.")
    
    with col2:
        st.markdown("**Recent IOC Lookups:**")
        recent_lookups = [
            "203.0.113.45 - Malicious IP",
            "malware.exe - Known Malware",
            "evil-domain.com - C2 Server"
        ]
        for lookup in recent_lookups:
            st.write(f"‚Ä¢ {lookup}")

def show_network_analysis():
    """Display network analysis interface"""
    st.markdown("### üåê Network Analysis")
    st.markdown("*Real-time network traffic monitoring and analysis*")
    
    # Initialize network data
    if 'network_connections' not in st.session_state:
        st.session_state.network_connections = [
            {
                'Timestamp': '2024-01-27 13:45:22',
                'Source IP': '192.168.1.100',
                'Destination IP': '203.0.113.45',
                'Source Port': '49152',
                'Dest Port': '443',
                'Protocol': 'HTTPS',
                'Bytes Sent': '1,245,000',
                'Bytes Received': '45,890',
                'Status': 'Suspicious',
                'Geolocation': 'Russia'
            },
            {
                'Timestamp': '2024-01-27 13:42:15',
                'Source IP': '192.168.1.105',
                'Destination IP': '8.8.8.8',
                'Source Port': '53241',
                'Dest Port': '53',
                'Protocol': 'DNS',
                'Bytes Sent': '128',
                'Bytes Received': '256',
                'Status': 'Normal',
                'Geolocation': 'USA'
            },
            {
                'Timestamp': '2024-01-27 13:40:08',
                'Source IP': '192.168.1.200',
                'Destination IP': '10.0.0.50',
                'Source Port': '3389',
                'Dest Port': '3389',
                'Protocol': 'RDP',
                'Bytes Sent': '2,540',
                'Bytes Received': '8,920',
                'Status': 'Normal',
                'Geolocation': 'Internal'
            },
            {
                'Timestamp': '2024-01-27 13:38:45',
                'Source IP': '192.168.1.150',
                'Destination IP': '185.220.101.42',
                'Source Port': '52341',
                'Dest Port': '9050',
                'Protocol': 'TOR',
                'Bytes Sent': '4,520',
                'Bytes Received': '12,850',
                'Status': 'Blocked',
                'Geolocation': 'Netherlands'
            }
        ]
    
    if 'network_stats' not in st.session_state:
        st.session_state.network_stats = {
            'total_connections': 1247,
            'suspicious_connections': 23,
            'blocked_connections': 8,
            'bandwidth_usage': '2.3 GB',
            'top_talkers': [
                {'IP': '192.168.1.100', 'Traffic': '1.2 GB'},
                {'IP': '192.168.1.105', 'Traffic': '890 MB'},
                {'IP': '192.168.1.200', 'Traffic': '456 MB'}
            ]
        }
    
    # Network Dashboard
    st.markdown("#### üìä Network Dashboard")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("Total Connections", st.session_state.network_stats['total_connections'])
    
    with col2:
        st.metric("Suspicious", st.session_state.network_stats['suspicious_connections'], 
                 delta=f"+{st.session_state.network_stats['suspicious_connections']}")
    
    with col3:
        st.metric("Blocked", st.session_state.network_stats['blocked_connections'])
    
    with col4:
        st.metric("Bandwidth Used", st.session_state.network_stats['bandwidth_usage'])
    
    # Real-time Monitoring Toggle
    st.markdown("---")
    col1, col2, col3 = st.columns(3)
    
    with col1:
        monitoring_enabled = st.checkbox("Real-time Monitoring", value=True)
        if monitoring_enabled:
            st.success("üî¥ Live monitoring active")
        else:
            st.warning("‚è∏Ô∏è Monitoring paused")
    
    with col2:
        auto_block = st.checkbox("Auto-block Suspicious IPs", value=False)
        if auto_block:
            st.info("üö´ Auto-blocking enabled")
    
    with col3:
        log_level = st.selectbox("Log Level", ['All Traffic', 'Suspicious Only', 'Blocked Only'])
    
    # Network Traffic Analysis
    st.markdown("---")
    st.markdown("#### üîç Live Network Connections")
    
    # Filter options
    col1, col2, col3 = st.columns(3)
    
    with col1:
        status_filter = st.selectbox("Filter by Status", ['All', 'Normal', 'Suspicious', 'Blocked'])
    
    with col2:
        protocol_filter = st.selectbox("Filter by Protocol", ['All', 'HTTPS', 'HTTP', 'DNS', 'RDP', 'SSH', 'TOR'])
    
    with col3:
        if st.button("üîÑ Refresh Data"):
            st.success("Network data refreshed!")
            st.rerun()
    
    # Apply filters
    filtered_connections = st.session_state.network_connections.copy()
    
    if status_filter != 'All':
        filtered_connections = [conn for conn in filtered_connections if conn['Status'] == status_filter]
    
    if protocol_filter != 'All':
        filtered_connections = [conn for conn in filtered_connections if conn['Protocol'] == protocol_filter]
    
    # Display connections table
    if filtered_connections:
        connections_df = pd.DataFrame(filtered_connections)
        st.dataframe(connections_df, use_container_width=True)
    else:
        st.info("No connections match the current filters.")
    
    # Network Analysis Tools
    st.markdown("---")
    st.markdown("#### üîß Network Analysis Tools")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("##### üîç IP Analysis")
        
        target_ip = st.text_input("Analyze IP Address", placeholder="e.g., 203.0.113.45")
        
        if st.button("üîç Analyze IP"):
            if target_ip:
                # Simulate IP analysis
                ip_analysis = {
                    'IP Address': target_ip,
                    'Geolocation': 'Russia, Moscow',
                    'ISP': 'Example ISP Ltd',
                    'Reputation': 'Malicious',
                    'Risk Score': '95/100',
                    'Associated Threats': ['Malware C2', 'Botnet'],
                    'First Seen': '2024-01-20',
                    'Last Activity': '2024-01-27 13:45:22',
                    'Connection Count': 15,
                    'Data Transferred': '1.2 GB'
                }
                
                with st.expander(f"Analysis Results for {target_ip}", expanded=True):
                    col_a, col_b = st.columns(2)
                    
                    with col_a:
                        st.write(f"**Geolocation:** {ip_analysis['Geolocation']}")
                        st.write(f"**ISP:** {ip_analysis['ISP']}")
                        st.write(f"**Reputation:** {ip_analysis['Reputation']}")
                        st.write(f"**Risk Score:** {ip_analysis['Risk Score']}")
                    
                    with col_b:
                        st.write(f"**Connection Count:** {ip_analysis['Connection Count']}")
                        st.write(f"**Data Transferred:** {ip_analysis['Data Transferred']}")
                        st.write(f"**First Seen:** {ip_analysis['First Seen']}")
                        st.write(f"**Last Activity:** {ip_analysis['Last Activity']}")
                    
                    st.write(f"**Associated Threats:** {', '.join(ip_analysis['Associated Threats'])}")
                    
                    if ip_analysis['Reputation'] == 'Malicious':
                        st.error("‚ö†Ô∏è This IP is flagged as malicious!")
                        
                        if st.button(f"üö´ Block {target_ip}"):
                            st.success(f"IP {target_ip} has been added to the block list.")
            else:
                st.error("Please enter an IP address to analyze.")
    
    with col2:
        st.markdown("##### üìà Traffic Pattern Analysis")
        
        analysis_type = st.selectbox(
            "Analysis Type",
            ['Top Talkers', 'Protocol Distribution', 'Geographic Distribution', 'Anomaly Detection']
        )
        
        if st.button("üìà Run Analysis"):
            if analysis_type == 'Top Talkers':
                st.markdown("**Top Traffic Sources:**")
                for i, talker in enumerate(st.session_state.network_stats['top_talkers']):
                    st.write(f"{i+1}. {talker['IP']} - {talker['Traffic']}")
            
            elif analysis_type == 'Protocol Distribution':
                protocol_data = {
                    'Protocol': ['HTTPS', 'HTTP', 'DNS', 'RDP', 'SSH', 'Other'],
                    'Percentage': [45, 25, 15, 8, 4, 3]
                }
                protocol_df = pd.DataFrame(protocol_data)
                
                fig = px.pie(protocol_df, values='Percentage', names='Protocol', 
                           title='Network Protocol Distribution')
                st.plotly_chart(fig, use_container_width=True)
            
            elif analysis_type == 'Geographic Distribution':
                geo_data = {
                    'Country': ['USA', 'Internal', 'Germany', 'Russia', 'China', 'Others'],
                    'Connections': [450, 320, 180, 120, 95, 82]
                }
                geo_df = pd.DataFrame(geo_data)
                
                fig = px.bar(geo_df, x='Country', y='Connections',
                           title='Connections by Geographic Location')
                st.plotly_chart(fig, use_container_width=True)
            
            elif analysis_type == 'Anomaly Detection':
                st.markdown("**Detected Network Anomalies:**")
                anomalies = [
                    "üî¥ Unusual data exfiltration pattern detected from 192.168.1.100",
                    "üî¥ Multiple failed connection attempts to external IPs",
                    "üü† High bandwidth usage outside business hours",
                    "üü° New device connected to network: 192.168.1.250"
                ]
                for anomaly in anomalies:
                    st.write(f"‚Ä¢ {anomaly}")
    
    # Network Security Rules
    st.markdown("---")
    st.markdown("#### üö´ Network Security Rules")
    
    if 'firewall_rules' not in st.session_state:
        st.session_state.firewall_rules = [
            {'Rule ID': 'FW-001', 'Action': 'Block', 'Source': '203.0.113.0/24', 'Protocol': 'Any', 'Status': 'Active'},
            {'Rule ID': 'FW-002', 'Action': 'Allow', 'Source': '192.168.1.0/24', 'Protocol': 'HTTPS', 'Status': 'Active'},
            {'Rule ID': 'FW-003', 'Action': 'Block', 'Source': 'Any', 'Protocol': 'TOR', 'Status': 'Active'}
        ]
    
    # Display current rules
    rules_df = pd.DataFrame(st.session_state.firewall_rules)
    st.dataframe(rules_df, use_container_width=True)
    
    # Add new rule
    with st.expander("Add New Firewall Rule"):
        col1, col2, col3 = st.columns(3)
        
        with col1:
            rule_action = st.selectbox("Action", ['Block', 'Allow', 'Monitor'])
            rule_source = st.text_input("Source IP/Range", placeholder="e.g., 192.168.1.0/24")
        
        with col2:
            rule_protocol = st.selectbox("Protocol", ['Any', 'HTTP', 'HTTPS', 'DNS', 'RDP', 'SSH', 'TOR'])
            rule_priority = st.slider("Priority", 1, 100, 50)
        
        with col3:
            rule_description = st.text_input("Description", placeholder="Rule description...")
        
        if st.button("‚ûï Add Rule"):
            if rule_source:
                new_rule = {
                    'Rule ID': f'FW-{len(st.session_state.firewall_rules) + 1:03d}',
                    'Action': rule_action,
                    'Source': rule_source,
                    'Protocol': rule_protocol,
                    'Priority': rule_priority,
                    'Status': 'Active',
                    'Description': rule_description
                }
                st.session_state.firewall_rules.append(new_rule)
                st.success(f"‚úÖ Firewall rule {new_rule['Rule ID']} added successfully!")
                st.rerun()
            else:
                st.error("Please specify a source IP or range.")
    
    # Network Topology Visualization
    st.markdown("---")
    st.markdown("#### üåê Network Topology")
    
    topology_data = {
        'Device': ['Firewall', 'Core Switch', 'Web Server', 'Database Server', 'Workstation-01', 'Workstation-02'],
        'IP Address': ['192.168.1.1', '192.168.1.2', '192.168.1.10', '192.168.1.20', '192.168.1.100', '192.168.1.105'],
        'Type': ['Security', 'Network', 'Server', 'Server', 'Endpoint', 'Endpoint'],
        'Status': ['Online', 'Online', 'Online', 'Online', 'Suspicious', 'Online'],
        'Connections': [1245, 890, 456, 234, 123, 89]
    }
    
    topology_df = pd.DataFrame(topology_data)
    
    # Color code based on status
    def get_status_color(status):
        if status == 'Online':
            return 'üü¢'
        elif status == 'Suspicious':
            return 'üî¥'
        else:
            return 'üü°'
    
    topology_df['Status_Icon'] = topology_df['Status'].apply(get_status_color)
    
    st.dataframe(topology_df[['Status_Icon', 'Device', 'IP Address', 'Type', 'Status', 'Connections']], 
                use_container_width=True)

def show_ioc_management():
    """Display IoC management interface"""
    st.markdown("### üéØ IoC Management")
    st.markdown("*Indicators of Compromise (IoC) tracking and management*")
    
    # Initialize IoC data
    if 'iocs' not in st.session_state:
        st.session_state.iocs = [
            {
                'IoC ID': 'IOC-001',
                'Type': 'IP Address',
                'Value': '203.0.113.45',
                'Threat Level': 'High',
                'Status': 'Active',
                'First Seen': '2024-01-20 10:30',
                'Last Seen': '2024-01-27 13:45',
                'Source': 'Threat Intel Feed',
                'Campaign': 'APT-X',
                'Description': 'Known C2 server for advanced persistent threat',
                'Actions Taken': 'Blocked at firewall'
            },
            {
                'IoC ID': 'IOC-002',
                'Type': 'File Hash',
                'Value': 'd41d8cd98f00b204e9800998ecf8427e',
                'Threat Level': 'Critical',
                'Status': 'Active',
                'First Seen': '2024-01-25 14:20',
                'Last Seen': '2024-01-27 12:15',
                'Source': 'Internal Detection',
                'Campaign': 'Ransomware-2024',
                'Description': 'Ransomware payload hash',
                'Actions Taken': 'Quarantined on endpoints'
            },
            {
                'IoC ID': 'IOC-003',
                'Type': 'Domain',
                'Value': 'malicious-domain.com',
                'Threat Level': 'Medium',
                'Status': 'Monitoring',
                'First Seen': '2024-01-22 09:15',
                'Last Seen': '2024-01-26 16:30',
                'Source': 'OSINT',
                'Campaign': 'Phishing Campaign',
                'Description': 'Domain used in phishing emails',
                'Actions Taken': 'DNS blocking implemented'
            },
            {
                'IoC ID': 'IOC-004',
                'Type': 'Email',
                'Value': 'attacker@evil-domain.com',
                'Threat Level': 'Medium',
                'Status': 'Resolved',
                'First Seen': '2024-01-15 11:45',
                'Last Seen': '2024-01-18 14:20',
                'Source': 'Email Security',
                'Campaign': 'Spear Phishing',
                'Description': 'Sender address in targeted phishing attacks',
                'Actions Taken': 'Email filtering rules updated'
            }
        ]
    
    # IoC Dashboard
    st.markdown("#### üìä IoC Dashboard")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        total_iocs = len(st.session_state.iocs)
        st.metric("Total IoCs", total_iocs)
    
    with col2:
        active_iocs = len([ioc for ioc in st.session_state.iocs if ioc['Status'] == 'Active'])
        st.metric("Active IoCs", active_iocs, delta=f"+{active_iocs}")
    
    with col3:
        critical_iocs = len([ioc for ioc in st.session_state.iocs if ioc['Threat Level'] == 'Critical'])
        st.metric("Critical IoCs", critical_iocs)
    
    with col4:
        recent_iocs = len([ioc for ioc in st.session_state.iocs if '2024-01-27' in ioc['Last Seen']])
        st.metric("Recent Activity", recent_iocs)
    
    # Add New IoC
    st.markdown("---")
    st.markdown("#### ‚ûï Add New IoC")
    
    with st.expander("Create New Indicator of Compromise"):
        col1, col2 = st.columns(2)
        
        with col1:
            ioc_type = st.selectbox(
                "IoC Type", 
                ['IP Address', 'Domain', 'URL', 'File Hash (MD5)', 'File Hash (SHA1)', 
                 'File Hash (SHA256)', 'Email', 'Registry Key', 'Mutex', 'User Agent']
            )
            ioc_value = st.text_input(
                "IoC Value", 
                placeholder="e.g., 192.168.1.100 or malware.exe or evil-domain.com"
            )
            threat_level = st.selectbox("Threat Level", ['Low', 'Medium', 'High', 'Critical'])
        
        with col2:
            ioc_source = st.selectbox(
                "Source", 
                ['Internal Detection', 'Threat Intel Feed', 'OSINT', 'Incident Response', 
                 'Third Party', 'Sandbox Analysis', 'Manual Entry']
            )
            campaign = st.text_input("Associated Campaign", placeholder="e.g., APT-X, Ransomware-2024")
            description = st.text_area("Description", placeholder="Describe the indicator and its context...")
        
        tags_input = st.text_input(
            "Tags (comma-separated)", 
            placeholder="e.g., malware, c2, phishing"
        )
        
        if st.button("üéØ Add IoC"):
            if ioc_value and description:
                from datetime import datetime
                
                new_ioc = {
                    'IoC ID': f'IOC-{len(st.session_state.iocs) + 1:03d}',
                    'Type': ioc_type,
                    'Value': ioc_value,
                    'Threat Level': threat_level,
                    'Status': 'Active',
                    'First Seen': datetime.now().strftime('%Y-%m-%d %H:%M'),
                    'Last Seen': datetime.now().strftime('%Y-%m-%d %H:%M'),
                    'Source': ioc_source,
                    'Campaign': campaign if campaign else 'Unknown',
                    'Description': description,
                    'Actions Taken': 'Pending review',
                    'Tags': tags_input.split(',') if tags_input else []
                }
                
                st.session_state.iocs.append(new_ioc)
                st.success(f"‚úÖ IoC {new_ioc['IoC ID']} added successfully!")
                st.rerun()
            else:
                st.error("Please provide both an IoC value and description.")
    
    # IoC Management Table
    st.markdown("---")
    st.markdown("#### üìã Current IoCs")
    
    # Filter options
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        type_filter = st.selectbox(
            "Filter by Type", 
            ['All'] + list(set([ioc['Type'] for ioc in st.session_state.iocs]))
        )
    
    with col2:
        threat_filter = st.selectbox("Filter by Threat Level", ['All', 'Critical', 'High', 'Medium', 'Low'])
    
    with col3:
        status_filter = st.selectbox("Filter by Status", ['All', 'Active', 'Monitoring', 'Resolved'])
    
    with col4:
        source_filter = st.selectbox(
            "Filter by Source", 
            ['All'] + list(set([ioc['Source'] for ioc in st.session_state.iocs]))
        )
    
    # Apply filters
    filtered_iocs = st.session_state.iocs.copy()
    
    if type_filter != 'All':
        filtered_iocs = [ioc for ioc in filtered_iocs if ioc['Type'] == type_filter]
    
    if threat_filter != 'All':
        filtered_iocs = [ioc for ioc in filtered_iocs if ioc['Threat Level'] == threat_filter]
    
    if status_filter != 'All':
        filtered_iocs = [ioc for ioc in filtered_iocs if ioc['Status'] == status_filter]
    
    if source_filter != 'All':
        filtered_iocs = [ioc for ioc in filtered_iocs if ioc['Source'] == source_filter]
    
    # Display filtered IoCs
    if filtered_iocs:
        iocs_df = pd.DataFrame(filtered_iocs)
        display_columns = ['IoC ID', 'Type', 'Value', 'Threat Level', 'Status', 'Last Seen', 'Source']
        st.dataframe(iocs_df[display_columns], use_container_width=True)
    else:
        st.info("No IoCs match the current filters.")
    
    # IoC Details and Management
    st.markdown("---")
    st.markdown("#### üîç IoC Details & Management")
    
    if st.session_state.iocs:
        selected_ioc_id = st.selectbox(
            "Select IoC for detailed view:",
            [ioc['IoC ID'] for ioc in st.session_state.iocs]
        )
        
        selected_ioc = next(ioc for ioc in st.session_state.iocs if ioc['IoC ID'] == selected_ioc_id)
        
        with st.expander(f"Details for {selected_ioc_id}", expanded=True):
            col1, col2 = st.columns(2)
            
            with col1:
                st.write(f"**Type:** {selected_ioc['Type']}")
                st.write(f"**Value:** `{selected_ioc['Value']}`")
                st.write(f"**Threat Level:** {selected_ioc['Threat Level']}")
                st.write(f"**Status:** {selected_ioc['Status']}")
                st.write(f"**Source:** {selected_ioc['Source']}")
            
            with col2:
                st.write(f"**Campaign:** {selected_ioc['Campaign']}")
                st.write(f"**First Seen:** {selected_ioc['First Seen']}")
                st.write(f"**Last Seen:** {selected_ioc['Last Seen']}")
                st.write(f"**Actions Taken:** {selected_ioc['Actions Taken']}")
            
            st.write(f"**Description:** {selected_ioc['Description']}")
            
            # Update IoC status
            col_a, col_b, col_c = st.columns(3)
            
            with col_a:
                new_status = st.selectbox(
                    "Update Status:",
                    ['Active', 'Monitoring', 'Resolved', 'False Positive'],
                    index=['Active', 'Monitoring', 'Resolved', 'False Positive'].index(selected_ioc['Status'])
                )
            
            with col_b:
                new_threat_level = st.selectbox(
                    "Update Threat Level:",
                    ['Critical', 'High', 'Medium', 'Low'],
                    index=['Critical', 'High', 'Medium', 'Low'].index(selected_ioc['Threat Level'])
                )
            
            with col_c:
                actions_taken = st.text_input(
                    "Actions Taken:",
                    value=selected_ioc['Actions Taken']
                )
            
            if st.button(f"Update {selected_ioc_id}"):
                # Update the IoC
                for i, ioc in enumerate(st.session_state.iocs):
                    if ioc['IoC ID'] == selected_ioc_id:
                        st.session_state.iocs[i]['Status'] = new_status
                        st.session_state.iocs[i]['Threat Level'] = new_threat_level
                        st.session_state.iocs[i]['Actions Taken'] = actions_taken
                        st.session_state.iocs[i]['Last Seen'] = datetime.now().strftime('%Y-%m-%d %H:%M')
                        break
                
                st.success(f"IoC {selected_ioc_id} updated successfully!")
                st.rerun()
    
    # IoC Intelligence Lookup
    st.markdown("---")
    st.markdown("#### üåê IoC Intelligence Lookup")
    
    col1, col2 = st.columns(2)
    
    with col1:
        lookup_value = st.text_input(
            "Lookup IoC Value", 
            placeholder="Enter IP, domain, hash, or email to lookup"
        )
        
        if st.button("üîç Lookup Intelligence"):
            if lookup_value:
                # Simulate intelligence lookup
                intel_results = {
                    'Value': lookup_value,
                    'Reputation': 'Malicious',
                    'Confidence': 'High (85%)',
                    'First Seen': '2024-01-15',
                    'Last Seen': '2024-01-27',
                    'Threat Types': ['Malware', 'C2 Communication'],
                    'Associated Campaigns': ['APT-X', 'Operation Shadow'],
                    'Geolocation': 'Russia, Moscow',
                    'ASN': 'AS12345 - Example ISP',
                    'WHOIS': 'Registered 2023-12-01',
                    'Sandbox Reports': ['Report 1', 'Report 2'],
                    'Related IoCs': ['192.168.1.200', 'evil-backup.com']
                }
                
                with st.expander(f"Intelligence Results for {lookup_value}", expanded=True):
                    col_x, col_y = st.columns(2)
                    
                    with col_x:
                        st.write(f"**Reputation:** {intel_results['Reputation']}")
                        st.write(f"**Confidence:** {intel_results['Confidence']}")
                        st.write(f"**Threat Types:** {', '.join(intel_results['Threat Types'])}")
                        st.write(f"**First/Last Seen:** {intel_results['First Seen']} / {intel_results['Last Seen']}")
                    
                    with col_y:
                        st.write(f"**Geolocation:** {intel_results['Geolocation']}")
                        st.write(f"**ASN:** {intel_results['ASN']}")
                        st.write(f"**WHOIS:** {intel_results['WHOIS']}")
                        st.write(f"**Campaigns:** {', '.join(intel_results['Associated Campaigns'])}")
                    
                    st.write(f"**Related IoCs:** {', '.join(intel_results['Related IoCs'])}")
                    
                    if intel_results['Reputation'] == 'Malicious':
                        st.error("‚ö†Ô∏è This indicator is flagged as malicious!")
                        
                        if st.button(f"Add {lookup_value} to IoC Database"):
                            # Add to IoC database
                            new_ioc = {
                                'IoC ID': f'IOC-{len(st.session_state.iocs) + 1:03d}',
                                'Type': 'IP Address' if '.' in lookup_value else 'Domain',
                                'Value': lookup_value,
                                'Threat Level': 'High',
                                'Status': 'Active',
                                'First Seen': intel_results['First Seen'],
                                'Last Seen': intel_results['Last Seen'],
                                'Source': 'Threat Intelligence Lookup',
                                'Campaign': intel_results['Associated Campaigns'][0],
                                'Description': f"Malicious indicator from intelligence lookup: {', '.join(intel_results['Threat Types'])}",
                                'Actions Taken': 'Added from intelligence lookup'
                            }
                            
                            st.session_state.iocs.append(new_ioc)
                            st.success(f"IoC {new_ioc['IoC ID']} added to database!")
                            st.rerun()
            else:
                st.error("Please enter an IoC value to lookup.")
    
    with col2:
        st.markdown("**Recent Intelligence Lookups:**")
        recent_intel_lookups = [
            "203.0.113.45 - Malicious C2 Server",
            "evil-domain.com - Phishing Domain", 
            "d41d8cd98f00b204e9800998ecf8427e - Malware Hash",
            "attacker@bad-domain.com - Phishing Email"
        ]
        
        for lookup in recent_intel_lookups:
            st.write(f"‚Ä¢ {lookup}")
    
    # IoC Statistics and Analytics
    st.markdown("---")
    st.markdown("#### üìà IoC Analytics")
    
    col1, col2 = st.columns(2)
    
    with col1:
        # IoC Distribution by Type
        type_counts = {}
        for ioc in st.session_state.iocs:
            ioc_type = ioc['Type']
            type_counts[ioc_type] = type_counts.get(ioc_type, 0) + 1
        
        if type_counts:
            type_df = pd.DataFrame(list(type_counts.items()), columns=['Type', 'Count'])
            fig = px.pie(type_df, values='Count', names='Type', title='IoC Distribution by Type')
            st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        # Threat Level Distribution
        threat_counts = {}
        for ioc in st.session_state.iocs:
            threat_level = ioc['Threat Level']
            threat_counts[threat_level] = threat_counts.get(threat_level, 0) + 1
        
        if threat_counts:
            threat_df = pd.DataFrame(list(threat_counts.items()), columns=['Threat Level', 'Count'])
            fig = px.bar(threat_df, x='Threat Level', y='Count', title='IoCs by Threat Level')
            st.plotly_chart(fig, use_container_width=True)
    
    # Bulk IoC Operations
    st.markdown("---")
    st.markdown("#### üìã Bulk Operations")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        bulk_iocs = st.text_area(
            "Bulk Add IoCs (one per line)",
            placeholder="203.0.113.45\nevil-domain.com\nd41d8cd98f00b204e9800998ecf8427e"
        )
        
        if st.button("‚ûï Bulk Add IoCs"):
            if bulk_iocs:
                ioc_lines = [line.strip() for line in bulk_iocs.split('\n') if line.strip()]
                added_count = 0
                
                for ioc_value in ioc_lines:
                    # Determine type
                    if '.' in ioc_value and len(ioc_value.split('.')) == 4:
                        ioc_type = 'IP Address'
                    elif '.' in ioc_value:
                        ioc_type = 'Domain' 
                    elif len(ioc_value) == 32:
                        ioc_type = 'File Hash (MD5)'
                    elif len(ioc_value) == 40:
                        ioc_type = 'File Hash (SHA1)'
                    elif len(ioc_value) == 64:
                        ioc_type = 'File Hash (SHA256)'
                    else:
                        ioc_type = 'Unknown'
                    
                    new_ioc = {
                        'IoC ID': f'IOC-{len(st.session_state.iocs) + added_count + 1:03d}',
                        'Type': ioc_type,
                        'Value': ioc_value,
                        'Threat Level': 'Medium',
                        'Status': 'Active',
                        'First Seen': datetime.now().strftime('%Y-%m-%d %H:%M'),
                        'Last Seen': datetime.now().strftime('%Y-%m-%d %H:%M'),
                        'Source': 'Bulk Import',
                        'Campaign': 'Unknown',
                        'Description': 'Bulk imported IoC',
                        'Actions Taken': 'Pending review'
                    }
                    
                    st.session_state.iocs.append(new_ioc)
                    added_count += 1
                
                st.success(f"‚úÖ Added {added_count} IoCs successfully!")
                st.rerun()
            else:
                st.error("Please enter IoCs to import.")
    
    with col2:
        if st.button("üìé Export All IoCs (CSV)"):
            import csv
            import io
            
            output = io.StringIO()
            writer = csv.DictWriter(output, fieldnames=st.session_state.iocs[0].keys())
            writer.writeheader()
            writer.writerows(st.session_state.iocs)
            
            st.download_button(
                label="üì• Download CSV",
                data=output.getvalue(),
                file_name=f"iocs_export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                mime="text/csv"
            )
    
    with col3:
        if st.button("üóëÔ∏è Clean Resolved IoCs"):
            original_count = len(st.session_state.iocs)
            st.session_state.iocs = [ioc for ioc in st.session_state.iocs if ioc['Status'] != 'Resolved']
            removed_count = original_count - len(st.session_state.iocs)
            
            if removed_count > 0:
                st.success(f"Removed {removed_count} resolved IoCs.")
                st.rerun()
            else:
                st.info("No resolved IoCs to remove.")

def show_seasonal_patterns():
    """Display seasonal patterns analysis"""
    st.markdown("### üìä Seasonal Patterns")
    st.info("Seasonal patterns analysis - Coming Soon!")

def show_forecasting():
    """Display forecasting interface"""
    st.markdown("### üîÆ Forecasting")
    st.info("Forecasting functionality - Coming Soon!")

def show_ts_anomaly_detection():
    """Display time series anomaly detection"""
    st.markdown("### üö® Time Series Anomaly Detection")
    st.info("Time series anomaly detection - Coming Soon!")

def show_risk_projection():
    """Display risk projection interface"""
    st.markdown("### üìà Risk Projection")
    st.info("Risk projection functionality - Coming Soon!")

def create_incident_response_dashboard(predictor):
    """Create incident response dashboard with alert creation functionality"""
    st.markdown("### üìã Incident Response & Reporting")
    
    # Initialize incidents in session state if not exists
    if 'incidents' not in st.session_state:
        st.session_state.incidents = [
            {
                'Incident ID': 'INC-2024-001', 'Type': 'Ransomware', 'Severity': 'Critical', 
                'Status': 'In Progress', 'Assigned To': 'Team A', 'Created': '2024-01-15 10:30',
                'Description': 'Ransomware attack detected on server infrastructure'
            },
            {
                'Incident ID': 'INC-2024-002', 'Type': 'Data Breach', 'Severity': 'High', 
                'Status': 'Resolved', 'Assigned To': 'Team B', 'Created': '2024-01-14 14:20',
                'Description': 'Unauthorized access to customer database'
            },
            {
                'Incident ID': 'INC-2024-003', 'Type': 'DDoS', 'Severity': 'Medium', 
                'Status': 'Investigating', 'Assigned To': 'Team C', 'Created': '2024-01-13 09:15',
                'Description': 'Distributed denial of service attack on web services'
            }
        ]
    
    # Initialize alerts if not exists (for integration)
    if 'alerts' not in st.session_state:
        st.session_state.alerts = []
    
    # Primary Incident/Alert Creation Form
    st.markdown("#### üö® Report New Incident/Alert")
    st.markdown("*Create incidents that automatically generate corresponding alerts for monitoring*")
    
    with st.form("incident_creation_form"):
        col1, col2 = st.columns(2)
        
        with col1:
            inc_type = st.selectbox(
                "Incident Type", 
                ['Intrusion', 'Phishing', 'Malware', 'DDoS', 'Data Breach', 'Ransomware', 
                 'Unauthorized Access', 'Suspicious Activity', 'Policy Violation', 'Other']
            )
            inc_severity = st.selectbox("Severity Level", ['Critical', 'High', 'Medium', 'Low'])
            inc_status = st.selectbox("Initial Status", ['Active', 'Investigating'])
        
        with col2:
            assigned_to = st.selectbox(
                "Assign To", 
                ['Team A - Critical Response', 'Team B - Investigation', 'Team C - Analysis', 
                 'Team D - Forensics', 'Auto-Assign']
            )
            estimated_impact = st.selectbox(
                "Estimated Impact", 
                ['High', 'Medium', 'Low', 'Unknown']
            )
            urgency = st.selectbox("Urgency", ['Immediate', 'High', 'Normal', 'Low'])
        
        inc_description = st.text_area(
            "Incident Description", 
            placeholder="Provide detailed description of the incident, including what was observed, when it occurred, and any immediate actions taken..."
        )
        
        additional_notes = st.text_area(
            "Additional Notes (Optional)", 
            placeholder="Any additional information, evidence, or context..."
        )
        
        # Form submission
        submitted = st.form_submit_button("üö® Create Incident & Alert", type="primary")
        
        if submitted:
            if inc_description.strip():  # Require description
                from datetime import datetime
                
                # Create incident ID
                incident_count = len(st.session_state.incidents)
                new_incident_id = f"INC-2024-{incident_count + 1:03d}"
                
                # Create alert ID  
                alert_count = len(st.session_state.alerts)
                new_alert_id = f"A{alert_count + 1:03d}"
                
                current_time = datetime.now().strftime('%Y-%m-%d %H:%M')
                
                # Create incident record
                new_incident = {
                    'Incident ID': new_incident_id,
                    'Type': inc_type,
                    'Severity': inc_severity,
                    'Status': inc_status,
                    'Assigned To': assigned_to,
                    'Created': current_time,
                    'Description': inc_description,
                    'Additional Notes': additional_notes,
                    'Impact': estimated_impact,
                    'Urgency': urgency,
                    'Related Alert': new_alert_id
                }
                
                # Create corresponding alert for monitoring
                new_alert = {
                    'Alert ID': new_alert_id,
                    'Type': inc_type,
                    'Severity': inc_severity,
                    'Status': inc_status,
                    'Resolved': False,
                    'Created': current_time,
                    'Description': inc_description,
                    'Related Incident': new_incident_id,
                    'Assigned To': assigned_to
                }
                
                # Add to session state
                st.session_state.incidents.append(new_incident)
                st.session_state.alerts.append(new_alert)
                
                # Success message
                st.success(f"‚úÖ **Incident {new_incident_id} created successfully!**")
                st.success(f"‚úÖ **Alert {new_alert_id} generated and sent to Alert Management**")
                st.info(f"üìù **Assigned to:** {assigned_to}")
                st.info(f"üîÑ **You can now view and manage this alert in:** üö® Alert Management")
                
                # Auto-refresh to show new data
                st.rerun()
            else:
                st.error("‚ö†Ô∏è Please provide a description for the incident.")
    
    # Incident Response Workflow
    st.markdown("---")
    st.markdown("#### üîÑ Incident Response Workflow")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.markdown("**1. üîç Detection**")
        st.info("Identify and report security incidents through this system")
    
    with col2:
        st.markdown("**2. üîç Analysis**")
        st.info("Investigate and assess threat impact and scope")
    
    with col3:
        st.markdown("**3. ‚öôÔ∏è Response**")
        st.info("Execute containment and mitigation strategies")
    
    with col4:
        st.markdown("**4. üîÑ Recovery**")
        st.info("Restore systems and document lessons learned")
    
    # Current Incidents Table
    st.markdown("---")
    st.markdown("#### üìà Current Incidents")
    
    if st.session_state.incidents:
        # Create DataFrame from incidents
        incidents_df = pd.DataFrame(st.session_state.incidents)
        
        # Display main columns
        display_columns = ['Incident ID', 'Type', 'Severity', 'Status', 'Assigned To', 'Created']
        st.dataframe(incidents_df[display_columns], use_container_width=True)
        
        # Incident details expander
        st.markdown("#### üîç Incident Details")
        selected_incident = st.selectbox(
            "Select incident to view details:", 
            [inc['Incident ID'] for inc in st.session_state.incidents]
        )
        
        if selected_incident:
            incident_details = next(inc for inc in st.session_state.incidents if inc['Incident ID'] == selected_incident)
            
            with st.expander(f"Details for {selected_incident}", expanded=True):
                col1, col2 = st.columns(2)
                
                with col1:
                    st.write(f"**Type:** {incident_details['Type']}")
                    st.write(f"**Severity:** {incident_details['Severity']}")
                    st.write(f"**Status:** {incident_details['Status']}")
                    st.write(f"**Created:** {incident_details['Created']}")
                
                with col2:
                    st.write(f"**Assigned To:** {incident_details['Assigned To']}")
                    st.write(f"**Impact:** {incident_details.get('Impact', 'Not specified')}")
                    st.write(f"**Urgency:** {incident_details.get('Urgency', 'Not specified')}")
                    st.write(f"**Related Alert:** {incident_details.get('Related Alert', 'None')}")
                
                st.write(f"**Description:** {incident_details['Description']}")
                
                if incident_details.get('Additional Notes'):
                    st.write(f"**Additional Notes:** {incident_details['Additional Notes']}")
    else:
        st.info("No incidents recorded yet. Create your first incident above.")
    
    # Incident Response Metrics
    st.markdown("---")
    st.markdown("#### üìâ Response Metrics")
    
    total_incidents = len(st.session_state.incidents)
    active_incidents = len([inc for inc in st.session_state.incidents if inc['Status'] in ['Active', 'Investigating']])
    critical_incidents = len([inc for inc in st.session_state.incidents if inc['Severity'] == 'Critical'])
    resolved_incidents = len([inc for inc in st.session_state.incidents if inc['Status'] == 'Resolved'])
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("Total Incidents", total_incidents)
    with col2:
        st.metric("Active Incidents", active_incidents)
    with col3:
        st.metric("Critical Incidents", critical_incidents)
    with col4:
        resolution_rate = f"{(resolved_incidents / total_incidents * 100):.1f}%" if total_incidents > 0 else "0%"
        st.metric("Resolution Rate", resolution_rate)

def show_system_config_dashboard():
    """Display system configuration dashboard"""
    st.markdown("### ‚öôÔ∏è System Configuration")
    
    # System settings
    st.markdown("#### System Settings")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("**Security Settings**")
        auto_retrain = st.checkbox("Enable Auto-Retraining", value=True)
        threat_intel = st.checkbox("Enable Threat Intelligence", value=True)
        real_time_monitoring = st.checkbox("Real-time Monitoring", value=True)
        
        st.markdown("**Alert Settings**")
        email_alerts = st.checkbox("Email Alerts", value=True)
        sms_alerts = st.checkbox("SMS Alerts", value=False)
        slack_integration = st.checkbox("Slack Integration", value=False)
    
    with col2:
        st.markdown("**Performance Settings**")
        batch_size = st.slider("Batch Processing Size", 100, 1000, 500)
        refresh_interval = st.slider("Dashboard Refresh (seconds)", 5, 60, 30)
        log_level = st.selectbox("Log Level", ['DEBUG', 'INFO', 'WARNING', 'ERROR'])
        
        st.markdown("**Data Retention**")
        data_retention_days = st.slider("Data Retention (days)", 30, 365, 90)
        log_retention_days = st.slider("Log Retention (days)", 7, 90, 30)
    
    # System status
    st.markdown("#### System Status")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("System Health", "98%", "2%")
    with col2:
        st.metric("Active Models", "4", "0")
    with col3:
        st.metric("Data Sources", "3", "0")
    with col4:
        st.metric("API Status", "Online", "0")
    
    # Save configuration
    if st.button("Save Configuration", type="primary"):
        st.success("Configuration saved successfully!")
    
    # System information
    st.markdown("#### System Information")
    
    system_info = {
        'Component': ['Python Version', 'Streamlit Version', 'Database', 'Last Updated', 'Uptime'],
        'Value': ['3.8+', '1.28+', 'SQLite', '2024-01-15', '5 days 12 hours'],
        'Status': ['‚úÖ OK', '‚úÖ OK', '‚úÖ Connected', '‚úÖ Recent', '‚úÖ Stable']
    }
    
    system_df = pd.DataFrame(system_info)
    st.dataframe(system_df, use_container_width=True)

if __name__ == "__main__":
    main()
